{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Recipe Reviews and User Feedback Dataset\" is a comprehensive repository of data encompassing various aspects of recipe reviews and user interactions. It includes essential information such as the recipe name, its ranking on the top 100 recipes list, a unique recipe code, and user details like user ID, user name, and an internal user reputation score. Each review comment is uniquely identified with a comment ID and comes with additional attributes, including the creation timestamp, reply count, and the number of up-votes and down-votes received. Users' sentiment towards recipes is quantified on a 1 to 5 star rating scale, with a score of 0 denoting an absence of rating. This dataset is a valuable resource for researchers and data scientists, facilitating endeavors in sentiment analysis, user behavior analysis, recipe recommendation systems, and more. It offers a window into the dynamics of recipe reviews and user feedback within the culinary website domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. recipe name: {name of the recipe the comment was posted on}\n",
    "2. recipe number: {placement of the recipe on the top 100 recipes list}\n",
    "3. recipe code: {unique id of the recipe used by the site}\n",
    "4. comment id: {unique id of the comment}\n",
    "5. user id: {unique id of the user who left the comment}\n",
    "6. user name: {name of the user}\n",
    "7. user reputation: {internal score of the site, roughly quantifying the past behavior of the user}\n",
    "8. create at: {time at which the comment was posted as a Unix timestamp}\n",
    "9. reply count: {number of replies to the comment}\n",
    "10. thumbs up: {number of up-votes the comment has received}\n",
    "11. thumbs down: {number of down-votes the comment has received}\n",
    "12. stars: {the score on a 1 to 5 scale that the user gave to the recipe. A score of 0 means that no score was given}\n",
    "13. best score: {score of the comment, likely used by the site the help determine the order in the comments that appear in}\n",
    "14. text: {the text content of the comment}\n",
    "\n",
    "unnamed 0: # of times that the recipe was reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/mn42899/data_scienceii/refs/heads/main/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'created_at' column from UNIX timestamp to a readable datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], unit='s')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the approval rating as a percentage\n",
    "df['approval_rating'] = (df['thumbs_up'] / (df['thumbs_up'] + df['thumbs_down'])) * 100\n",
    "\n",
    "# Replace NaN values (e.g., when thumbs_up and thumbs_down are both 0) with 0\n",
    "df['approval_rating'] = df['approval_rating'].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviews for the specific recipe\n",
    "specific_recipe = \"Mamaw Emily’s Strawberry Cake\"\n",
    "\n",
    "# Ensure the recipe_name column exists and count the entries\n",
    "if 'recipe_name' in df.columns:\n",
    "    recipe_count = df[df['recipe_name'] == specific_recipe].shape[0]\n",
    "    print(f\"Number of reviews for '{specific_recipe}': {recipe_count}\")\n",
    "else:\n",
    "    print(\"The 'recipe_name' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by recipe_name and count duplicates in the text column\n",
    "duplicates_per_recipe = (\n",
    "    df[df.duplicated(subset=['recipe_name', 'text','user_name' ], keep=False)]  # Filter duplicates\n",
    "    .groupby('recipe_name')\n",
    "    .size()\n",
    "    .reset_index(name='duplicate_count')  # Rename the count column\n",
    ")\n",
    "\n",
    "# Display the counts of duplicates per recipe\n",
    "print(duplicates_per_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on recipe_name and text, keeping the first occurrence\n",
    "df.drop_duplicates(subset=['recipe_name', 'text'], keep='first', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Ensure all values in the 'text' column are strings\n",
    "df['text'] = df['text'].fillna('').astype(str)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function for text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing directly to the 'text' column\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Optional: Print the number of rows to verify\n",
    "print(f\"Number of rows after dropping null values: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted histogram for 'best_score' with more bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['best_score'], bins=50, edgecolor='black')  # Increased bins to 50\n",
    "plt.title('Histogram of Best Scores (Adjusted Bin Size)', fontsize=16)\n",
    "plt.xlabel('Best Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum and maximum dates\n",
    "min_date = df['created_at'].min()\n",
    "max_date = df['created_at'].max()\n",
    "\n",
    "print(f\"Minimum date: {min_date}\")\n",
    "print(f\"Maximum date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by month and count reviews\n",
    "df['month'] = df['created_at'].dt.to_period('M')\n",
    "monthly_review_distribution = df.groupby('month').size()\n",
    "\n",
    "# Plot the monthly distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_review_distribution.index.astype(str), monthly_review_distribution.values, marker='o')\n",
    "plt.title('Review Distribution Over Time (Monthly)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Number of Reviews', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rep = df['user_reputation'].min()\n",
    "max_rep = df['user_reputation'].max()\n",
    "\n",
    "print(f\"Minimum Rep: {min_rep}\")\n",
    "print(f\"Maximum Rep: {max_rep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_reputation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for 'user_reputation'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['user_reputation'], bins=100, edgecolor='black')\n",
    "plt.title('User Reputation Distribution', fontsize=16)\n",
    "plt.xlabel('User Reputation', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by recipe_number and compute summary statistics\n",
    "summary_stats = df.groupby('recipe_number').agg(\n",
    "    total_comments=('comment_id', 'count'),       # Count of comments\n",
    "    avg_user_reputation=('user_reputation', 'mean'),  # Average user reputation\n",
    "    avg_reply_count=('reply_count', 'mean'),     # Average reply count\n",
    "    avg_thumbs_up=('thumbs_up', 'mean'),         # Average thumbs up\n",
    "    avg_thumbs_down=('thumbs_down', 'mean'),     # Average thumbs down\n",
    "    avg_star_rating=('stars', 'mean'),           # Average star rating\n",
    "    avg_best_score=('best_score', 'mean')        # Average best score\n",
    ").reset_index()\n",
    "\n",
    "# Display the results\n",
    "summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ratio column for thumbs up to thumbs down\n",
    "summary_stats['thumbs_up_to_down_ratio'] = summary_stats['avg_thumbs_up'] / summary_stats['avg_thumbs_down']\n",
    "\n",
    "# Handle cases where thumbs down is 0 to avoid division by zero\n",
    "summary_stats['thumbs_up_to_down_ratio'] = summary_stats['thumbs_up_to_down_ratio'].replace([float('inf'), -float('inf')], None).fillna(0)\n",
    "\n",
    "# Display the updated summary table\n",
    "summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recipes = df.groupby(['recipe_number', 'recipe_name']).size().reset_index(name='total_comments')\n",
    "top10 = top_recipes.sort_values(by='recipe_number', ascending=True)\n",
    "\n",
    "top10.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_volume = df.groupby(df['created_at'].dt.to_period('M')).size().reset_index(name='comment_count')\n",
    "\n",
    "plt.plot(comment_volume['created_at'].dt.to_timestamp(), comment_volume['comment_count'])\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Comment Volume over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of comments occur at 10/11 am (Before lunch time to prep the recipe???)\n",
    "\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "\n",
    "hourly_distribution = df.groupby('hour').size().reset_index(name='comment_count')\n",
    "\n",
    "#Plot the distribution\n",
    "plt.bar(hourly_distribution['hour'], hourly_distribution['comment_count'])\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Distribution of Comments by Time of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the comments were happening in June\n",
    "\n",
    "# Group by recipe_number and week\n",
    "recipe_timeline = df.groupby([df['created_at'].dt.to_period('W'), 'recipe_number']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot the activity of top recipes\n",
    "recipe_timeline.plot(figsize=(10, 6))\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Activity of Top Recipes Over Time')\n",
    "plt.legend(title='Recipe Number', bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and calculate averages\n",
    "metrics_trend = df.groupby(df['created_at'].dt.to_period('D'))[['reply_count', 'thumbs_up', 'thumbs_down']].mean().reset_index()\n",
    "\n",
    "# Plot each metric over time\n",
    "for metric in ['reply_count', 'thumbs_up', 'thumbs_down']:\n",
    "    plt.plot(metrics_trend['created_at'].dt.to_timestamp(), metrics_trend[metric], label=metric)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Average Metric')\n",
    "plt.title('Metrics Trend Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataset by 'thumbs_up' in descending order\n",
    "top_thumbs_up_comments = df.sort_values(by='thumbs_up', ascending=False)\n",
    "\n",
    "# Select relevant columns to display (including recipe_number and the full text comment)\n",
    "top_comments = top_thumbs_up_comments[['recipe_number', 'text', 'thumbs_up']].head(30)\n",
    "\n",
    "# Display the top comments with the highest thumbs up\n",
    "top_comments.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the ratio of 5 stars compared to other stars for each recipe\n",
    "\n",
    "# Count total ratings for each recipe\n",
    "total_counts = df.groupby('recipe_number').size().reset_index(name='total_ratings')\n",
    "\n",
    "# Count 5-star ratings for each recipe\n",
    "five_star_counts = df[df['stars'] == 5].groupby('recipe_number').size().reset_index(name='five_star_count')\n",
    "\n",
    "# Merge total ratings and 5-star ratings\n",
    "merged_counts = pd.merge(total_counts, five_star_counts, on='recipe_number', how='left')\n",
    "\n",
    "# Fill NaN values in five_star_count (for recipes without any 5-star ratings) with 0\n",
    "merged_counts['five_star_count'] = merged_counts['five_star_count'].fillna(0)\n",
    "\n",
    "# Calculate the ratio of 5-star ratings to total ratings\n",
    "merged_counts['five_star_ratio'] = merged_counts['five_star_count'] / merged_counts['total_ratings']\n",
    "\n",
    "# Display the results\n",
    "print(\"Five-Star Ratio for Each Recipe:\")\n",
    "print(merged_counts[['recipe_number', 'five_star_ratio']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical features and textual data provide complementary information that improves the sentiment classification (negative, neutral, or positive). Here’s how they interact and affect the sentiment analysis:\n",
    "\n",
    "1. Textual Data (TF-IDF)\n",
    "\n",
    "Textual data captures the semantic content of reviews. Words or phrases often carry direct sentiment information. For example:\n",
    "\t•\tWords like “bad,” “terrible,” or “awful” indicate negative sentiment.\n",
    "\t•\tWords like “okay” or “average” suggest neutral sentiment.\n",
    "\t•\tWords like “excellent,” “amazing,” or “perfect” imply positive sentiment.\n",
    "\n",
    "By converting this data into TF-IDF vectors, the model learns the importance of specific words in determining the sentiment:\n",
    "\t•\tHigh TF-IDF scores for positive words (e.g., “great,” “love”) lead to higher chances of predicting the review as positive.\n",
    "\t•\tSimilarly, high TF-IDF scores for negative words (e.g., “poor,” “disappointing”) guide the model toward a negative classification.\n",
    "\n",
    "\n",
    "2. Numerical Features\n",
    "\n",
    "Numerical features like approval_rating, user_reputation, reply_count, and best_score provide additional context about the review or reviewer that can influence sentiment prediction:\n",
    "\n",
    "(a) Approval Rating (approval_rating):\n",
    "\t•\tA higher approval rating could correlate with a more positive sentiment.\n",
    "\t•\tExample: A user who consistently receives high approval ratings for their reviews is likely to write more positive reviews overall.\n",
    "\n",
    "(b) User Reputation (user_reputation):\n",
    "\t•\tA reviewer with high reputation might have a more critical tone, even in neutral or positive reviews.\n",
    "\t•\tExample: A high-reputation user might be more likely to give nuanced feedback, which could alter the sentiment classification.\n",
    "\n",
    "(c) Reply Count (reply_count):\n",
    "\t•\tReviews with a high reply count might generate stronger sentiments (either positive or negative), as they are more likely to elicit reactions.\n",
    "\t•\tExample: A review with many replies could indicate a controversial or strongly worded opinion.\n",
    "\n",
    "(d) Best Score (best_score):\n",
    "\t•\tA high best_score might indicate an exceptional review, likely aligning with positive sentiments.\n",
    "\t•\tExample: Reviews with high scores are more likely to be rated positively by other users, implying a positive tone.\n",
    "\n",
    "\n",
    "3. Effect on the 3 Sentiment Classes\n",
    "\n",
    "When combined, the numerical features and textual data affect the classification as follows:\n",
    "\n",
    "Positive Sentiment (4–5 stars)\n",
    "\t•\tText: Words like “excellent,” “love,” or “fantastic” increase the probability of predicting a positive sentiment.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tHigh approval_rating reinforces positivity.\n",
    "\t•\tHigh reply_count or best_score adds confidence in the prediction.\n",
    "\n",
    "Neutral Sentiment (3 stars)\n",
    "\t•\tText: Words like “okay,” “average,” or “fine” suggest neutrality.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tModerate approval_rating or user_reputation can help identify neutral reviews.\n",
    "\t•\tLow reply_count might indicate less polarizing content.\n",
    "\n",
    "Negative Sentiment (0–2 stars)\n",
    "\t•\tText: Words like “terrible,” “disappointing,” or “poor” increase the likelihood of negative sentiment classification.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tLow approval_rating and low best_score further emphasize negative sentiment.\n",
    "\n",
    "4. How Combined Features Improve Classification\n",
    "\n",
    "The combination of textual and numerical features creates a richer representation of the reviews:\n",
    "\t•\tTextual Data provides context and sentiment-rich signals.\n",
    "\t•\tNumerical Features add contextual metadata that may influence sentiment indirectly.\n",
    "\n",
    "Example:\n",
    "\n",
    "A review says, “The product is okay but arrived late.”\n",
    "\t•\tText Analysis:\n",
    "\t•\tWords like “okay” indicate neutrality, but “arrived late” leans toward negative.\n",
    "\t•\tNumerical Data:\n",
    "\t•\tIf the approval_rating is low and reply_count is high, the classifier might predict a negative sentiment, despite the text being borderline neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset (replace with your actual dataset if needed)\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])  # Ensure no missing values\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment, with 0 stars as neutral\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "# Apply the updated function\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels as integers\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Preprocess text, numerical features, and labels\n",
    "texts = df['text'].astype(str).tolist()\n",
    "approval_ratings = df['approval_rating'].tolist()\n",
    "user_reputations = df['user_reputation'].tolist()\n",
    "reply_counts = df['reply_count'].tolist()\n",
    "best_scores = df['best_score'].tolist()\n",
    "labels = df['sentiment_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preprocessing: Use the enhanced sentiment analysis setup\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])  # Ensure no missing values\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment_classifier'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels as integers\n",
    "label_mapping_classifier = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded_classifier'] = df['sentiment_classifier'].map(label_mapping_classifier)\n",
    "\n",
    "# Split dataset\n",
    "X_text_classifier = df['text'].astype(str)\n",
    "X_features_classifier = df[['approval_rating', 'user_reputation', 'reply_count', 'best_score']].copy()\n",
    "y_classifier = df['sentiment_encoded_classifier']\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train_text_classifier, X_test_text_classifier, X_train_features_classifier, X_test_features_classifier, y_train_classifier, y_test_classifier = train_test_split(\n",
    "    X_text_classifier, X_features_classifier, y_classifier, test_size=0.2, random_state=42, stratify=y_classifier\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorizer for text\n",
    "tfidf_vectorizer_classifier = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf_classifier = tfidf_vectorizer_classifier.fit_transform(X_train_text_classifier).toarray()\n",
    "X_test_tfidf_classifier = tfidf_vectorizer_classifier.transform(X_test_text_classifier).toarray()\n",
    "\n",
    "# Normalize numerical features\n",
    "X_train_features_classifier = X_train_features_classifier / X_train_features_classifier.max()\n",
    "X_test_features_classifier = X_test_features_classifier / X_train_features_classifier.max()\n",
    "\n",
    "# Combine text and numerical features\n",
    "X_train_combined_classifier = np.hstack((X_train_tfidf_classifier, X_train_features_classifier))\n",
    "X_test_combined_classifier = np.hstack((X_test_tfidf_classifier, X_test_features_classifier))\n",
    "\n",
    "# Model-Specific Features\n",
    "models_classifier = {\n",
    "    \"Random Forest\": {\n",
    "        \"features_train\": X_train_combined_classifier,\n",
    "        \"features_test\": X_test_combined_classifier,\n",
    "        \"model\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"features_train\": X_train_tfidf_classifier,  # Only text features\n",
    "        \"features_test\": X_test_tfidf_classifier,\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "    },\n",
    "    \"Support Vector Machine (SVM)\": {\n",
    "        \"features_train\": X_train_features_classifier,  # Only numerical features\n",
    "        \"features_test\": X_test_features_classifier,\n",
    "        \"model\": SVC(kernel='linear', random_state=42)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and Evaluate Models\n",
    "results_classifier = {}\n",
    "for model_name, model_info in models_classifier.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model_classifier = model_info[\"model\"]\n",
    "    X_train_classifier = model_info[\"features_train\"]\n",
    "    X_test_classifier = model_info[\"features_test\"]\n",
    "\n",
    "    # Train the model\n",
    "    model_classifier.fit(X_train_classifier, y_train_classifier)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_classifier = model_classifier.predict(X_test_classifier)\n",
    "    accuracy_classifier = model_classifier.score(X_test_classifier, y_test_classifier)\n",
    "    results_classifier[model_name] = accuracy_classifier\n",
    "\n",
    "    print(f\"{model_name} Accuracy: {accuracy_classifier}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_classifier, y_pred_classifier, target_names=label_mapping_classifier.keys()))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_classifier = confusion_matrix(y_test_classifier, y_pred_classifier)\n",
    "    sns.heatmap(cm_classifier, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping_classifier.keys(), yticklabels=label_mapping_classifier.keys())\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Summary of Model Accuracies\n",
    "print(\"\\nModel Accuracy Summary:\")\n",
    "for model_name, accuracy in results_classifier.items():\n",
    "    print(f\"{model_name}: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest Classifier:\n",
    "\t•\tLayers:\n",
    "\t•\tDecision Trees: A Random Forest consists of multiple decision trees (e.g., n_estimators=100 means 100 trees are used).\n",
    "\t•\tVoting Mechanism: Each tree makes a prediction, and the forest aggregates the results (e.g., majority vote for classification).\n",
    "\t•\tKey Components:\n",
    "\t•\tDecision splits are made at each tree node based on features.\n",
    "\t•\tPredictions are aggregated across all trees in the forest.\n",
    "\n",
    "\n",
    "2. Logistic Regression:\n",
    "\t•\tLayers:\n",
    "\t•\tInput Layer: Accepts the TF-IDF text features as input.\n",
    "\t•\tLinear Transformation: Applies a linear equation  y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots  to calculate probabilities for each class.\n",
    "\t•\tSigmoid Function (Binary) or Softmax Function (Multiclass): Converts the linear outputs into probabilities.\n",
    "\t•\tKey Components:\n",
    "\t•\tUses a probabilistic model to separate classes by fitting a hyperplane in the feature space.\n",
    "\n",
    "3. Support Vector Machine (SVM):\n",
    "\t•\tLayers:\n",
    "\t•\tFeature Transformation: Maps the input features (numerical in this case) into a high-dimensional space.\n",
    "\t•\tHyperplane: Finds the optimal hyperplane that separates classes with the largest possible margin.\n",
    "\t•\tSupport Vectors: Identifies the data points closest to the hyperplane, which influence its position.\n",
    "\t•\tKey Components:\n",
    "\t•\tFocuses on maximizing the margin between classes.\n",
    "\t•\tWorks well for linear and non-linear decision boundaries depending on the kernel used (linear in this case).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Callback to Monitor F1 Score\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, patience=2):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y_true = self.validation_data\n",
    "        val_y_pred_probs = self.model.predict(val_x)\n",
    "        val_y_pred = np.argmax(val_y_pred_probs, axis=1)\n",
    "        current_f1 = f1_score(val_y_true, val_y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Macro F1 Score = {current_f1:.4f}\")\n",
    "        \n",
    "        # Save the best weights if F1 improves\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(\"Early stopping triggered due to no improvement in Macro F1 Score.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "bidir_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "bidir_df = bidir_df[bidir_df['stars'].between(0, 5)]\n",
    "\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "bidir_df['sentiment'] = bidir_df['stars'].apply(map_star_to_sentiment)\n",
    "bidir_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "bidir_df['sentiment_encoded'] = bidir_df['sentiment'].map(bidir_label_mapping)\n",
    "\n",
    "bidir_texts = bidir_df['text'].astype(str).tolist()\n",
    "bidir_labels = bidir_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Additional numerical features\n",
    "bidir_approval = bidir_df['approval_rating'].tolist()\n",
    "bidir_reputation = bidir_df['user_reputation'].tolist()\n",
    "bidir_reply = bidir_df['reply_count'].tolist()\n",
    "bidir_score = bidir_df['best_score'].tolist()\n",
    "\n",
    "# Split data\n",
    "bidir_X_train_text, bidir_X_test_text, bidir_y_train, bidir_y_test, bidir_X_train_approval, bidir_X_test_approval, \\\n",
    "bidir_X_train_reputation, bidir_X_test_reputation, bidir_X_train_reply, bidir_X_test_reply, \\\n",
    "bidir_X_train_score, bidir_X_test_score = train_test_split(\n",
    "    bidir_texts, bidir_labels, bidir_approval, bidir_reputation, bidir_reply, bidir_score, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "bidir_X_train_approval = np.array(bidir_X_train_approval).reshape(-1, 1) / 100.0\n",
    "bidir_X_test_approval = np.array(bidir_X_test_approval).reshape(-1, 1) / 100.0\n",
    "bidir_X_train_reputation = np.array(bidir_X_train_reputation).reshape(-1, 1) / max(bidir_reputation)\n",
    "bidir_X_test_reputation = np.array(bidir_X_test_reputation).reshape(-1, 1) / max(bidir_reputation)\n",
    "bidir_X_train_reply = np.array(bidir_X_train_reply).reshape(-1, 1) / max(bidir_reply)\n",
    "bidir_X_test_reply = np.array(bidir_X_test_reply).reshape(-1, 1) / max(bidir_reply)\n",
    "bidir_X_train_score = np.array(bidir_X_train_score).reshape(-1, 1) / max(bidir_score)\n",
    "bidir_X_test_score = np.array(bidir_X_test_score).reshape(-1, 1) / max(bidir_score)\n",
    "\n",
    "bidir_X_train_numerical = np.hstack((\n",
    "    bidir_X_train_approval, bidir_X_train_reputation, bidir_X_train_reply, bidir_X_train_score\n",
    "))\n",
    "bidir_X_test_numerical = np.hstack((\n",
    "    bidir_X_test_approval, bidir_X_test_reputation, bidir_X_test_reply, bidir_X_test_score\n",
    "))\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "bidir_max_vocab_size = 5000\n",
    "bidir_max_sequence_length = 100\n",
    "\n",
    "bidir_tokenizer = Tokenizer(num_words=bidir_max_vocab_size, oov_token=\"<OOV>\")\n",
    "bidir_tokenizer.fit_on_texts(bidir_X_train_text)\n",
    "\n",
    "bidir_X_train_seq = bidir_tokenizer.texts_to_sequences(bidir_X_train_text)\n",
    "bidir_X_test_seq = bidir_tokenizer.texts_to_sequences(bidir_X_test_text)\n",
    "\n",
    "bidir_X_train_padded = pad_sequences(bidir_X_train_seq, maxlen=bidir_max_sequence_length, padding='post', truncating='post')\n",
    "bidir_X_test_padded = pad_sequences(bidir_X_test_seq, maxlen=bidir_max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the BiRNN model\n",
    "def build_bidir_model_with_numerical(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    # Text input and embedding\n",
    "    text_input = tf.keras.layers.Input(shape=(bidir_max_sequence_length,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=bidir_max_vocab_size, output_dim=64)(text_input)\n",
    "    birnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=False))(embedding)\n",
    "    \n",
    "    # Numerical features input\n",
    "    numerical_input = tf.keras.layers.Input(shape=(bidir_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([birnn, numerical_input])\n",
    "    \n",
    "    # Dense and output layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    outputs = tf.keras.layers.Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train and evaluate with hyperparameter tuning\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "units_list = [64]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "for units in units_list:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Testing configuration: units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "            \n",
    "            bidir_model = build_bidir_model_with_numerical(units, dropout_rate, learning_rate)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(\n",
    "                [bidir_X_test_padded, bidir_X_test_numerical], bidir_y_test\n",
    "            ), patience=2)\n",
    "            \n",
    "            history = bidir_model.fit(\n",
    "                [bidir_X_train_padded, bidir_X_train_numerical], np.array(bidir_y_train),\n",
    "                validation_data=(\n",
    "                    [bidir_X_test_padded, bidir_X_test_numerical], np.array(bidir_y_test)\n",
    "                ),\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                callbacks=[f1_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            bidir_y_pred_probs = bidir_model.predict([bidir_X_test_padded, bidir_X_test_numerical])\n",
    "            bidir_y_pred = np.argmax(bidir_y_pred_probs, axis=1)\n",
    "            macro_f1 = f1_score(bidir_y_test, bidir_y_pred, average='macro')\n",
    "            \n",
    "            if macro_f1 > best_f1_score:\n",
    "                best_f1_score = macro_f1\n",
    "                best_params = {\n",
    "                    'units': units,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "                best_model = bidir_model\n",
    "\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "bidir_y_pred = np.argmax(best_model.predict([bidir_X_test_padded, bidir_X_test_numerical]), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(bidir_y_test, bidir_y_pred))\n",
    "\n",
    "cm = confusion_matrix(bidir_y_test, bidir_y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Input Layer\n",
    "\t•\tInput Layer: Input(shape=(bidir_max_sequence_length,))\n",
    "\t•\tAccepts sequences of tokenized and padded text data.\n",
    "\t•\tShape: (None, bidir_max_sequence_length), where bidir_max_sequence_length is the maximum length of the padded sequences.\n",
    "\n",
    "2. Embedding Layer\n",
    "\t•\tEmbedding: Embedding(input_dim=bidir_max_vocab_size, output_dim=64)\n",
    "\t•\tMaps integer-encoded words into dense 64-dimensional vectors.\n",
    "\t•\tInput: Tokenized text (integer indices of words).\n",
    "\t•\tOutput: A 2D tensor (None, bidir_max_sequence_length, 64) where each word is represented by a 64-dimensional vector.\n",
    "\n",
    "3. Bidirectional RNN Layer\n",
    "\t•\tBidirectional Layer: Bidirectional(LSTM(units, return_sequences=False))\n",
    "\t•\tA BiRNN layer that applies a forward and backward LSTM.\n",
    "\t•\tunits: Number of LSTM units in each direction.\n",
    "\t•\treturn_sequences=False: Outputs the last hidden state (a single vector for each input sequence).\n",
    "\t•\tOutput: A vector of size 2 * units due to concatenation of forward and backward states.\n",
    "\n",
    "4. Numerical Input Layer\n",
    "\t•\tInput Layer: Input(shape=(bidir_X_train_numerical.shape[1],))\n",
    "\t•\tAccepts numerical features (e.g., approval rating, reputation).\n",
    "\t•\tShape: (None, num_numerical_features).\n",
    "\n",
    "5. Concatenation Layer\n",
    "\t•\tConcatenate: Concatenate()([birnn, numerical_input])\n",
    "\t•\tCombines the output of the BiRNN layer (text representation) with the numerical features.\n",
    "\t•\tOutput: A single concatenated vector.\n",
    "\n",
    "6. Dense Layer\n",
    "\t•\tDense Layer: Dense(units, activation='relu')\n",
    "\t•\tFully connected layer with units neurons.\n",
    "\t•\tActivation: ReLU (Rectified Linear Unit) to introduce non-linearity.\n",
    "\t•\tOutput: A vector of size units.\n",
    "\n",
    "7. Dropout Layer\n",
    "\t•\tDropout Layer: Dropout(dropout_rate)\n",
    "\t•\tRegularization to prevent overfitting by randomly setting a fraction (dropout_rate) of inputs to zero during training.\n",
    "\n",
    "8. Output Layer\n",
    "\t•\tDense Layer: Dense(5, activation='softmax')\n",
    "\t•\tFully connected layer with 5 neurons corresponding to the 5 classes (stars rating categories).\n",
    "\t•\tActivation: Softmax to output probabilities for each class.\n",
    "\n",
    "Summary of Input and Outputs:\n",
    "\t•\tInput 1: Text sequences (text_input).\n",
    "\t•\tInput 2: Numerical features (numerical_input).\n",
    "\t•\tOutput: Probabilities for each of the 5 classes.\n",
    "\n",
    "This architecture effectively combines text data (via embeddings and BiRNN) with structured numerical features, making it suitable for tasks where both types of data are important for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttBiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Attention Layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"attention_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W)\n",
    "        e = tf.keras.backend.squeeze(e, axis=-1)\n",
    "        alpha = tf.keras.backend.softmax(e)\n",
    "        alpha = tf.keras.backend.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = tf.keras.backend.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# Custom Callback to Monitor F1 Score\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, patience=2):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y_true = self.validation_data\n",
    "        val_y_pred_probs = self.model.predict(val_x)\n",
    "        val_y_pred = np.argmax(val_y_pred_probs, axis=1)\n",
    "        current_f1 = f1_score(val_y_true, val_y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Macro F1 Score = {current_f1:.4f}\")\n",
    "        \n",
    "        # Save the best weights if F1 improves\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(\"Early stopping triggered due to no improvement in Macro F1 Score.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "attbirnn_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "attbirnn_df = attbirnn_df[attbirnn_df['stars'].between(0, 5)]\n",
    "\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "attbirnn_df['sentiment'] = attbirnn_df['stars'].apply(map_star_to_sentiment)\n",
    "attbirnn_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "attbirnn_df['sentiment_encoded'] = attbirnn_df['sentiment'].map(attbirnn_label_mapping)\n",
    "\n",
    "attbirnn_texts = attbirnn_df['text'].astype(str).tolist()\n",
    "attbirnn_labels = attbirnn_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Additional numerical features\n",
    "attbirnn_approval = attbirnn_df['approval_rating'].tolist()\n",
    "attbirnn_reputation = attbirnn_df['user_reputation'].tolist()\n",
    "attbirnn_reply = attbirnn_df['reply_count'].tolist()\n",
    "attbirnn_score = attbirnn_df['best_score'].tolist()\n",
    "\n",
    "# Split data\n",
    "attbirnn_X_train_text, attbirnn_X_test_text, attbirnn_y_train, attbirnn_y_test, attbirnn_X_train_approval, attbirnn_X_test_approval, \\\n",
    "attbirnn_X_train_reputation, attbirnn_X_test_reputation, attbirnn_X_train_reply, attbirnn_X_test_reply, \\\n",
    "attbirnn_X_train_score, attbirnn_X_test_score = train_test_split(\n",
    "    attbirnn_texts, attbirnn_labels, attbirnn_approval, attbirnn_reputation, attbirnn_reply, attbirnn_score, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "attbirnn_X_train_approval = np.array(attbirnn_X_train_approval).reshape(-1, 1) / 100.0\n",
    "attbirnn_X_test_approval = np.array(attbirnn_X_test_approval).reshape(-1, 1) / 100.0\n",
    "attbirnn_X_train_reputation = np.array(attbirnn_X_train_reputation).reshape(-1, 1) / max(attbirnn_reputation)\n",
    "attbirnn_X_test_reputation = np.array(attbirnn_X_test_reputation).reshape(-1, 1) / max(attbirnn_reputation)\n",
    "attbirnn_X_train_reply = np.array(attbirnn_X_train_reply).reshape(-1, 1) / max(attbirnn_reply)\n",
    "attbirnn_X_test_reply = np.array(attbirnn_X_test_reply).reshape(-1, 1) / max(attbirnn_reply)\n",
    "attbirnn_X_train_score = np.array(attbirnn_X_train_score).reshape(-1, 1) / max(attbirnn_score)\n",
    "attbirnn_X_test_score = np.array(attbirnn_X_test_score).reshape(-1, 1) / max(attbirnn_score)\n",
    "\n",
    "attbirnn_X_train_numerical = np.hstack((\n",
    "    attbirnn_X_train_approval, attbirnn_X_train_reputation, attbirnn_X_train_reply, attbirnn_X_train_score\n",
    "))\n",
    "attbirnn_X_test_numerical = np.hstack((\n",
    "    attbirnn_X_test_approval, attbirnn_X_test_reputation, attbirnn_X_test_reply, attbirnn_X_test_score\n",
    "))\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "attbirnn_max_vocab_size = 5000\n",
    "attbirnn_max_sequence_length = 100\n",
    "\n",
    "attbirnn_tokenizer = Tokenizer(num_words=attbirnn_max_vocab_size, oov_token=\"<OOV>\")\n",
    "attbirnn_tokenizer.fit_on_texts(attbirnn_X_train_text)\n",
    "\n",
    "attbirnn_X_train_seq = attbirnn_tokenizer.texts_to_sequences(attbirnn_X_train_text)\n",
    "attbirnn_X_test_seq = attbirnn_tokenizer.texts_to_sequences(attbirnn_X_test_text)\n",
    "\n",
    "attbirnn_X_train_padded = pad_sequences(attbirnn_X_train_seq, maxlen=attbirnn_max_sequence_length, padding='post', truncating='post')\n",
    "attbirnn_X_test_padded = pad_sequences(attbirnn_X_test_seq, maxlen=attbirnn_max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the AttBiRNN model\n",
    "def build_attbirnn_model_with_numerical(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    # Text input and embedding\n",
    "    text_input = tf.keras.layers.Input(shape=(attbirnn_max_sequence_length,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=attbirnn_max_vocab_size, output_dim=64)(text_input)\n",
    "    birnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True))(embedding)\n",
    "    attention = Attention()(birnn)\n",
    "    \n",
    "    # Numerical features input\n",
    "    numerical_input = tf.keras.layers.Input(shape=(attbirnn_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([attention, numerical_input])\n",
    "    \n",
    "    # Dense and output layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    outputs = tf.keras.layers.Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and evaluate with hyperparameter tuning\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "units_list = [64]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "for units in units_list:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Testing configuration: units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "            \n",
    "            attbirnn_model = build_attbirnn_model_with_numerical(units, dropout_rate, learning_rate)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(\n",
    "                [attbirnn_X_test_padded, attbirnn_X_test_numerical], attbirnn_y_test\n",
    "            ), patience=2)\n",
    "            \n",
    "            history = attbirnn_model.fit(\n",
    "                [attbirnn_X_train_padded, attbirnn_X_train_numerical], np.array(attbirnn_y_train),\n",
    "                validation_data=(\n",
    "                    [attbirnn_X_test_padded, attbirnn_X_test_numerical], np.array(attbirnn_y_test)\n",
    "                ),\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                callbacks=[f1_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            attbirnn_y_pred_probs = attbirnn_model.predict([attbirnn_X_test_padded, attbirnn_X_test_numerical])\n",
    "            attbirnn_y_pred = np.argmax(attbirnn_y_pred_probs, axis=1)\n",
    "            macro_f1 = f1_score(attbirnn_y_test, attbirnn_y_pred, average='macro')\n",
    "            \n",
    "            if macro_f1 > best_f1_score:\n",
    "                best_f1_score = macro_f1\n",
    "                best_params = {\n",
    "                    'units': units,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "                best_model = attbirnn_model\n",
    "\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "attbirnn_y_pred = np.argmax(best_model.predict([attbirnn_X_test_padded, attbirnn_X_test_numerical]), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(attbirnn_y_test, attbirnn_y_pred))\n",
    "\n",
    "cm = confusion_matrix(attbirnn_y_test, attbirnn_y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text Input and Embedding\n",
    "\t1.\tText Input Layer\n",
    "\t•\tInput(shape=(attbirnn_max_sequence_length,))\n",
    "\t•\tAccepts tokenized and padded text sequences.\n",
    "\t•\tShape: (None, attbirnn_max_sequence_length).\n",
    "\t2.\tEmbedding Layer\n",
    "\t•\tEmbedding(input_dim=attbirnn_max_vocab_size, output_dim=64)\n",
    "\t•\tMaps integer-encoded words into dense 64-dimensional vectors.\n",
    "\t•\tInput: Tokenized text (integer indices of words).\n",
    "\t•\tOutput: (None, attbirnn_max_sequence_length, 64), where each word is a 64-dimensional vector.\n",
    "\n",
    "2. Bidirectional RNN with Attention\n",
    "\t3. Bidirectional RNN Layer\n",
    "\t•\tBidirectional(LSTM(units, return_sequences=True))\n",
    "\t•\tApplies an LSTM in both forward and backward directions.\n",
    "\t•\treturn_sequences=True: Outputs hidden states for every time step, enabling attention.\n",
    "\t•\tOutput: (None, attbirnn_max_sequence_length, 2 * units) (concatenated forward and backward states).\n",
    "\t4. Attention Layer\n",
    "\t•\tAttention()\n",
    "\t•\tComputes an attention score for each time step of the Bidirectional RNN output.\n",
    "\t•\tAttention mechanism highlights relevant parts of the text by weighting them.\n",
    "\t•\tWeights (alpha): Learnable attention weights for time steps.\n",
    "\t•\tContext Vector: Weighted sum of the RNN outputs across all time steps.\n",
    "\t•\tOutput: (None, 2 * units) (context vector summarizing the sequence).\n",
    "\n",
    "3. Numerical Features Input\n",
    "\t5. Numerical Input Layer\n",
    "\t•\tInput(shape=(attbirnn_X_train_numerical.shape[1],))\n",
    "\t•\tAccepts numerical features like approval_rating, user_reputation, etc.\n",
    "\t•\tShape: (None, num_numerical_features).\n",
    "\n",
    "4. Combining Text and Numerical Features\n",
    "\t6. Concatenation Layer\n",
    "\t•\tConcatenate()([attention, numerical_input])\n",
    "\t•\tCombines the context vector from the attention layer with numerical features.\n",
    "\t•\tOutput: A single vector of size 2 * units + num_numerical_features.\n",
    "\n",
    "5. Fully Connected Layers\n",
    "\t7. Dense Layer\n",
    "\t•\tDense(units, activation='relu')\n",
    "\t•\tFully connected layer with units neurons.\n",
    "\t•\tActivation: ReLU introduces non-linearity.\n",
    "\t•\tOutput: A vector of size units.\n",
    "\t8. Dropout Layer\n",
    "\t•\tDropout(dropout_rate)\n",
    "\t•\tRandomly sets a fraction of inputs to zero for regularization.\n",
    "\t\n",
    "6. Output Layer\n",
    "\t9. Dense Output Layer\n",
    "\t•\tDense(5, activation='softmax')\n",
    "\t•\tFully connected layer with 5 neurons, corresponding to 5 sentiment classes.\n",
    "\t•\tActivation: Softmax converts outputs into class probabilities.\n",
    "\t•\tOutput: Probability distribution over 5 classes.\n",
    "\n",
    "Summary of Inputs and Outputs\n",
    "\t•\tInput 1: Text sequences (text_input).\n",
    "\t•\tInput 2: Numerical features (numerical_input).\n",
    "\t•\tOutput: Probabilities for each of the 5 classes.\n",
    "\n",
    "Significance of the Attention Mechanism\n",
    "\n",
    "The attention layer enables the model to focus on the most relevant parts of the text sequence, effectively enhancing performance in tasks where specific words or phrases are critical for classification. This makes the model more interpretable and context-aware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Concatenate, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "# Clean and process the original DataFrame `df`\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "texts = df['text'].astype(str).tolist()\n",
    "approval_ratings = df['approval_rating'].tolist()\n",
    "user_reputations = df['user_reputation'].tolist()\n",
    "reply_counts = df['reply_count'].tolist()\n",
    "best_scores = df['best_score'].tolist()\n",
    "labels = df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, X_train_rating, X_test_rating, X_train_reputation, X_test_reputation, X_train_reply, X_test_reply, X_train_score, X_test_score, y_train_lstm_combined, y_test_lstm_combined = train_test_split(\n",
    "    texts, approval_ratings, user_reputations, reply_counts, best_scores, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize and Pad Text Data\n",
    "vocab_size = 5000\n",
    "max_seq_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_len, padding='post')\n",
    "\n",
    "# Normalize numerical features\n",
    "X_train_rating = np.array(X_train_rating).reshape(-1, 1) / 100.0\n",
    "X_test_rating = np.array(X_test_rating).reshape(-1, 1) / 100.0\n",
    "X_train_reputation = np.array(X_train_reputation).reshape(-1, 1) / df['user_reputation'].max()\n",
    "X_test_reputation = np.array(X_test_reputation).reshape(-1, 1) / df['user_reputation'].max()\n",
    "X_train_reply = np.array(X_train_reply).reshape(-1, 1) / df['reply_count'].max()\n",
    "X_test_reply = np.array(X_test_reply).reshape(-1, 1) / df['reply_count'].max()\n",
    "X_train_score = np.array(X_train_score).reshape(-1, 1) / df['best_score'].max()\n",
    "X_test_score = np.array(X_test_score).reshape(-1, 1) / df['best_score'].max()\n",
    "\n",
    "# Combine numerical features into a single array\n",
    "X_train_numerical = np.hstack((X_train_rating, X_train_reputation, X_train_reply, X_train_score))\n",
    "X_test_numerical = np.hstack((X_test_rating, X_test_reputation, X_test_reply, X_test_score))\n",
    "\n",
    "# Use a subset of the data for hyperparameter tuning\n",
    "subset_size = len(X_train_padded) // 2\n",
    "X_train_subset_text = X_train_padded[:subset_size]\n",
    "X_train_subset_num = X_train_numerical[:subset_size]\n",
    "y_train_subset_lstm_combined = y_train_lstm_combined[:subset_size]\n",
    "\n",
    "# Step 2: Define the Model for Tuning\n",
    "def build_lstm_model(hp):\n",
    "    text_input = Input(shape=(max_seq_len,), name=\"text_input\")\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=hp.Int(\"embedding_dim\", min_value=64, max_value=128, step=64),\n",
    "        input_length=max_seq_len\n",
    "    )(text_input)\n",
    "    lstm_layer = LSTM(\n",
    "        units=hp.Int(\"lstm_units\", min_value=64, max_value=128, step=64),\n",
    "        return_sequences=False\n",
    "    )(embedding_layer)\n",
    "    \n",
    "    numerical_input = Input(shape=(4,), name=\"numerical_input\")\n",
    "    concatenated = Concatenate()([lstm_layer, numerical_input])\n",
    "    \n",
    "    dense_layer = Dense(\n",
    "        units=hp.Int(\"dense_units\", min_value=64, max_value=128, step=64),\n",
    "        activation=\"relu\"\n",
    "    )(concatenated)\n",
    "    dropout_layer = Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.3, step=0.1))(dense_layer)\n",
    "    output_layer = Dense(3, activation=\"softmax\")(dropout_layer)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[text_input, numerical_input], outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-3, sampling=\"log\")\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 3: Hyperparameter Tuning with Keras Tuner\n",
    "temp_dir = tempfile.gettempdir()\n",
    "tuner = kt.Hyperband(\n",
    "    build_lstm_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=5,\n",
    "    factor=4,\n",
    "    directory=temp_dir,\n",
    "    project_name=\"lstm_sentiment_tuning_with_num\"\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    [X_train_subset_text, X_train_subset_num],\n",
    "    np.array(y_train_subset_lstm_combined),\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Get the Best Hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The optimal embedding dimension is {best_hps.get('embedding_dim')},\n",
    "with {best_hps.get('lstm_units')} LSTM units,\n",
    "{best_hps.get('dense_units')} units in the dense layer,\n",
    "dropout of {best_hps.get('dropout')},\n",
    "and learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Step 5: Train the Best Model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    [X_train_padded, X_train_numerical],\n",
    "    np.array(y_train_lstm_combined),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "test_loss_lstm_combined, test_accuracy_lstm_combined = best_model.evaluate(\n",
    "    [X_test_padded, X_test_numerical], \n",
    "    np.array(y_test_lstm_combined), \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm_combined}\")\n",
    "\n",
    "# Step 7: Predictions and Metrics\n",
    "y_pred_lstm_combined = np.argmax(best_model.predict([X_test_padded, X_test_numerical]), axis=1)\n",
    "\n",
    "# Compute macro F1-score\n",
    "macro_f1_lstm_combined = f1_score(y_test_lstm_combined, y_pred_lstm_combined, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1_lstm_combined}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lstm_combined = confusion_matrix(y_test_lstm_combined, y_pred_lstm_combined)\n",
    "sns.heatmap(cm_lstm_combined, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_lstm_combined, y_pred_lstm_combined, target_names=label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preparation\n",
    "\t•\tClean and Process Dataset:\n",
    "\t•\tRemoved rows with missing values in relevant columns: text, stars, approval_rating, user_reputation, reply_count, best_score.\n",
    "\t•\tFiltered rows to ensure stars values are between 0 and 5.\n",
    "\t•\tSentiment Mapping:\n",
    "\t•\tStars ≤ 2 → “negative”, Star = 3 → “neutral”, Stars ≥ 4 → “positive”.\n",
    "\t•\tEncoded sentiments into numerical values: 0 = negative, 1 = neutral, 2 = positive.\n",
    "\t•\tSplitting Data:\n",
    "\t•\tUsed train_test_split to separate data into training and testing sets.\n",
    "\t•\tExtracted and normalized additional numerical features: approval_rating, user_reputation, reply_count, best_score.\n",
    "\t•\tText Tokenization and Padding:\n",
    "\t•\tTokenized text data with a vocabulary size of 5000.\n",
    "\t•\tPadded sequences to a maximum length of 100 tokens.\n",
    "\n",
    "2. Model Definition\n",
    "\t•\tText Input and Embedding:\n",
    "            Input layer for tokenized text.\n",
    "            Embedding layer with a tunable dimension (64 to 128).\n",
    "\t•\tLSTM Layer:\n",
    "\t        Bidirectional LSTM with tunable units (64 to 128).\n",
    "\t        Outputs a representation of text data.\n",
    "\t•\tNumerical Input:\n",
    "\t        Separate input for numerical features (4 features normalized between 0 and 1).\n",
    "\t•\tDense Layers and Output:\n",
    "\t        Concatenation of LSTM output and numerical features.\n",
    "\t        Dense layer with tunable units (64 to 128) and dropout (0.2 to 0.3).\n",
    "\t        Output layer with 3 neurons (softmax for sentiment classification).\n",
    "\t\t\t\n",
    "3. Hyperparameter Tuning\n",
    "\t•\tUsed Keras Tuner (Hyperband) for automated hyperparameter search:\n",
    "\t•\tTunable parameters:\n",
    "\t•\tEmbedding dimension: 64 to 128.\n",
    "\t•\tLSTM units: 64 to 128.\n",
    "\t•\tDense layer units: 64 to 128.\n",
    "\t•\tDropout rate: 0.2 to 0.3.\n",
    "\t•\tLearning rate: 1e-4 to 1e-3.\n",
    "\t•\tObjective: Minimize validation loss.\n",
    "\t•\tPerformed search on a subset of the training data with validation split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification model built using a Deep Neural Network (DNN) architecture with an Embedding Layer and a Global Average Pooling Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Filter valid data\n",
    "df = df.dropna(subset=['text', 'stars'])\n",
    "df = df[df['stars'].between(1, 5)]  # Ensure stars values are between 1 and 5\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "dnn_texts = df['text'].astype(str)\n",
    "dnn_labels = df['stars']\n",
    "\n",
    "# Step 3: Split the dataset\n",
    "dnn_X_train, dnn_X_test, dnn_y_train, dnn_y_test = train_test_split(\n",
    "    dnn_texts, dnn_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Tokenize text data\n",
    "dnn_tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "dnn_tokenizer.fit_on_texts(dnn_X_train)\n",
    "\n",
    "dnn_X_train_seq = dnn_tokenizer.texts_to_sequences(dnn_X_train)\n",
    "dnn_X_test_seq = dnn_tokenizer.texts_to_sequences(dnn_X_test)\n",
    "\n",
    "# Step 5: Pad sequences\n",
    "dnn_max_len = 100\n",
    "dnn_X_train_padded = pad_sequences(dnn_X_train_seq, maxlen=dnn_max_len, padding='post', truncating='post')\n",
    "dnn_X_test_padded = pad_sequences(dnn_X_test_seq, maxlen=dnn_max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to 0-indexed (since stars are 1-5, subtract 1)\n",
    "dnn_y_train = dnn_y_train - 1\n",
    "dnn_y_test = dnn_y_test - 1\n",
    "\n",
    "# Step 6: Build the model\n",
    "dnn_num_classes = 5\n",
    "\n",
    "dnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add dropout for regularization\n",
    "    tf.keras.layers.Dense(dnn_num_classes, activation='softmax')  # Softmax for multi-class output\n",
    "])\n",
    "\n",
    "dnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # Multi-class loss function\n",
    "                  metrics=[])  # Remove accuracy to calculate F1-score separately\n",
    "\n",
    "# Step 7: Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=3,          # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "dnn_history = dnn_model.fit(\n",
    "    dnn_X_train_padded, \n",
    "    dnn_y_train, \n",
    "    epochs=20,  # Set a higher number of epochs\n",
    "    validation_data=(dnn_X_test_padded, dnn_y_test),\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "# Predict classes for the test set\n",
    "dnn_y_test_pred_probs = dnn_model.predict(dnn_X_test_padded)\n",
    "dnn_y_test_pred = tf.argmax(dnn_y_test_pred_probs, axis=1).numpy()\n",
    "\n",
    "# Classification Report\n",
    "dnn_class_report = classification_report(dnn_y_test, dnn_y_test_pred, target_names=[str(i) for i in range(1, 6)])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(dnn_class_report)\n",
    "\n",
    "# Calculate F1-score\n",
    "dnn_f1 = f1_score(dnn_y_test, dnn_y_test_pred, average='weighted')\n",
    "print(f\"Test F1-Score: {dnn_f1:.4f}\")\n",
    "\n",
    "# Step 10: Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dnn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(dnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.axvline(x=len(dnn_history.history['loss']) - early_stopping.patience - 1, color='red', linestyle='--', label='Early Stopping Triggered')\n",
    "plt.title('Model Loss with Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “loss” represents how far off the model’s predictions are from the true labels. Specifically:\n",
    "\t•\tTraining Loss: Measures the error the model makes on the training data.\n",
    "\t•\tValidation Loss: Measures the error the model makes on unseen (validation) data.\n",
    "    \n",
    "The graph shows that both training and validation loss decrease over time, meaning the model is learning effectively.\n",
    "\n",
    "The validation loss is slightly higher than the training loss, which is normal and indicates the model generalizes well. Since the losses flatten at the end and there’s no large gap, the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DNN has a sequential architecture, and its layers are as follows:\n",
    "1.\tEmbedding Layer:\n",
    "\t•\tInput: Sequences of integers representing tokenized words (from the Tokenizer).\n",
    "\t•\tFunction: Converts each word index into a dense vector of a fixed size (embedding dimension = 16 in this case).\n",
    "\t•\tPurpose: Captures semantic relationships between words in a continuous vector space.\n",
    "\t•\tLayer Details:\n",
    "\t•\tInput dimension: 10000 (vocabulary size).\n",
    "\t•\tOutput dimension: 16 (embedding size for each word).\n",
    "\n",
    "2.\tGlobal Average Pooling 1D Layer:\n",
    "\t•\tInput: The output of the Embedding layer (a 2D tensor with word embeddings).\n",
    "\t•\tFunction: Averages the embeddings across all words in a sequence, producing a single vector for each input sequence.\n",
    "\t•\tPurpose: Reduces the sequence into a fixed-size vector, regardless of the input length.\n",
    "\n",
    "3.\tDense Layer:\n",
    "\t•\tInput: The pooled embeddings.\n",
    "\t•\tFunction: Applies a fully connected layer with 16 neurons and a ReLU activation function.\n",
    "\t•\tPurpose: Learns higher-level feature representations of the input.\n",
    "\n",
    "4.\tDropout Layer:\n",
    "\t•\tFunction: Randomly sets 50% of the neurons to zero during training.\n",
    "\t•\tPurpose: Prevents overfitting by introducing regularization.\n",
    "\n",
    "5.\tDense Output Layer:\n",
    "\t•\tInput: The output from the Dense and Dropout layers.\n",
    "\t•\tFunction: Applies a fully connected layer with 5 neurons (one for each class) and a softmax activation.\n",
    "\t•\tPurpose: Outputs the probability distribution over the 5 classes (corresponding to star ratings 1 to 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Directional LSTM for Specific Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Prepare the DataFrame for the specific recipe\n",
    "df_specific = df.copy()\n",
    "\n",
    "# Filter for the specific recipe\n",
    "specific_recipe = \"Caramel Heavenlies\"\n",
    "df_specific = df_specific.dropna(subset=['text', 'stars', 'recipe_name'])\n",
    "df_specific = df_specific[df_specific['stars'].between(1, 5)]\n",
    "df_specific = df_specific[df_specific['recipe_name'] == specific_recipe]\n",
    "\n",
    "# Check if there are enough reviews for the recipe\n",
    "if df_specific.shape[0] < 1:\n",
    "    raise ValueError(f\"No reviews found for the recipe '{specific_recipe}'.\")\n",
    "\n",
    "# Preprocess the text and labels\n",
    "texts_specific = df_specific['text'].astype(str)\n",
    "labels_specific = df_specific['stars']\n",
    "\n",
    "# Handle small datasets\n",
    "if df_specific.shape[0] < 2:\n",
    "    print(f\"Warning: Only {df_specific.shape[0]} review(s) found for '{specific_recipe}'.\")\n",
    "    X_train, X_test, y_train, y_test = texts_specific, texts_specific, labels_specific, labels_specific\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts_specific, labels_specific, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 150\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to 0-indexed\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Step 2: Build and Train the LSTM Model\n",
    "num_classes = 5\n",
    "model_specific = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_specific.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=[])\n",
    "\n",
    "# Train the model\n",
    "history_specific = model_specific.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Step 3: Evaluate with F1-Score\n",
    "# Predict and generate predicted classes\n",
    "y_pred_probs = model_specific.predict(X_test_padded)\n",
    "y_pred_classes = y_pred_probs.argmax(axis=-1)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(f\"{specific_recipe} Model Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Step 4: Classification Report\n",
    "# Dynamically set target names based on unique classes\n",
    "unique_classes = sorted(y_test.unique())\n",
    "target_names = [str(label + 1) for label in unique_classes]  # Adjust to match star ratings\n",
    "\n",
    "print(f\"\\n{specific_recipe} Model Classification Report\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))\n",
    "\n",
    "# Step 5: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title(f'Confusion Matrix - {specific_recipe} Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Visualize Training and Validation Metrics\n",
    "# Training and Validation Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_specific.history['loss'], label='Training Loss')\n",
    "plt.plot(history_specific.history['val_loss'], label='Validation Loss')\n",
    "plt.title(f'{specific_recipe} Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tEmbedding Layer:\n",
    "\t•\tInput: Tokenized and padded sequences of text (each word represented as an integer).\n",
    "\t•\tFunction: Converts word indices into dense vectors of size 64.\n",
    "\t•\tPurpose: Encodes semantic relationships between words into continuous vector space.\n",
    "\t•\tParameters:\n",
    "\t•\tInput dimension: 10,000 (vocabulary size).\n",
    "\t•\tOutput dimension: 64 (embedding size for each word).\n",
    "\t•\tInput length: 150 (maximum sequence length).\n",
    "\n",
    "2.\tBidirectional LSTM Layer 1:\n",
    "\t•\tInput: Word embeddings for each sequence.\n",
    "\t•\tFunction: A Bidirectional LSTM processes the input sequence both forward and backward, capturing contextual information from both directions.\n",
    "\t•\tOutput: Sequences of hidden states (one for each timestep).\n",
    "\t•\tUnits: 64 LSTM units in each direction (128 combined).\n",
    "\n",
    "3.\tBidirectional LSTM Layer 2:\n",
    "\t•\tInput: The output of the first LSTM layer.\n",
    "\t•\tFunction: Similar to the first layer but returns only the final hidden states.\n",
    "\t•\tUnits: 32 LSTM units in each direction (64 combined).\n",
    "\t•\tPurpose: Further condenses sequence information into a single vector.\n",
    "\n",
    "4.\tDense Layer:\n",
    "\t•\tInput: The final hidden state from the second LSTM layer.\n",
    "\t•\tFunction: Fully connected layer with 32 neurons and ReLU activation.\n",
    "\t•\tPurpose: Learns high-level feature representations for classification.\n",
    "\n",
    "5.\tOutput Layer:\n",
    "\t•\tInput: The output of the dense layer.\n",
    "\t•\tFunction: Fully connected layer with 5 neurons and softmax activation.\n",
    "\t•\tPurpose: Outputs a probability distribution over the 5 classes (1 to 5 stars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_halving_search_cv  # Enable HalvingGridSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, make_scorer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "rf_model_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "rf_model_df = rf_model_df[rf_model_df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment categories\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "rf_model_df['sentiment'] = rf_model_df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels into integers\n",
    "rf_sentiment_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "rf_model_df['sentiment_encoded'] = rf_model_df['sentiment'].map(rf_sentiment_label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "rf_model_texts = rf_model_df['text'].astype(str).tolist()\n",
    "rf_model_approval_ratings = rf_model_df['approval_rating'].tolist()\n",
    "rf_model_user_reputation = rf_model_df['user_reputation'].tolist()\n",
    "rf_model_reply_count = rf_model_df['reply_count'].tolist()\n",
    "rf_model_best_score = rf_model_df['best_score'].tolist()\n",
    "rf_model_labels = rf_model_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "rf_train_texts, rf_test_texts, rf_train_approval_ratings, rf_test_approval_ratings, rf_train_reputation, rf_test_reputation, \\\n",
    "rf_train_reply_count, rf_test_reply_count, rf_train_best_score, rf_test_best_score, rf_train_labels, rf_test_labels = train_test_split(\n",
    "    rf_model_texts, rf_model_approval_ratings, rf_model_user_reputation, rf_model_reply_count, rf_model_best_score, rf_model_labels,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Text feature extraction using TF-IDF\n",
    "rf_tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "rf_train_tfidf_features = rf_tfidf_vectorizer.fit_transform(rf_train_texts).toarray()\n",
    "rf_test_tfidf_features = rf_tfidf_vectorizer.transform(rf_test_texts).toarray()\n",
    "\n",
    "# Normalize numerical features\n",
    "rf_normalized_train_approval_ratings = np.array(rf_train_approval_ratings).reshape(-1, 1) / 100.0\n",
    "rf_normalized_test_approval_ratings = np.array(rf_test_approval_ratings).reshape(-1, 1) / 100.0\n",
    "rf_normalized_train_reputation = np.array(rf_train_reputation).reshape(-1, 1) / max(rf_model_user_reputation)\n",
    "rf_normalized_test_reputation = np.array(rf_test_reputation).reshape(-1, 1) / max(rf_model_user_reputation)\n",
    "rf_normalized_train_reply_count = np.array(rf_train_reply_count).reshape(-1, 1) / max(rf_model_reply_count)\n",
    "rf_normalized_test_reply_count = np.array(rf_test_reply_count).reshape(-1, 1) / max(rf_model_reply_count)\n",
    "rf_normalized_train_best_score = np.array(rf_train_best_score).reshape(-1, 1) / max(rf_model_best_score)\n",
    "rf_normalized_test_best_score = np.array(rf_test_best_score).reshape(-1, 1) / max(rf_model_best_score)\n",
    "\n",
    "# Combine all features\n",
    "rf_final_train_features = np.hstack((\n",
    "    rf_train_tfidf_features,\n",
    "    rf_normalized_train_approval_ratings,\n",
    "    rf_normalized_train_reputation,\n",
    "    rf_normalized_train_reply_count,\n",
    "    rf_normalized_train_best_score\n",
    "))\n",
    "rf_final_test_features = np.hstack((\n",
    "    rf_test_tfidf_features,\n",
    "    rf_normalized_test_approval_ratings,\n",
    "    rf_normalized_test_reputation,\n",
    "    rf_normalized_test_reply_count,\n",
    "    rf_normalized_test_best_score\n",
    "))\n",
    "\n",
    "# Parameter grid for hyperparameter tuning\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Custom scorer for macro F1\n",
    "rf_macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# HalvingGridSearchCV for hyperparameter tuning\n",
    "rf_halving_grid_search = HalvingGridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    scoring=rf_macro_f1_scorer,  # Optimize for macro F1 score\n",
    "    factor=2,  # Controls the rate of resource reduction\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Early stopping mechanism for macro F1\n",
    "rf_best_macro_f1_score = 0\n",
    "rf_no_improvement_count = 0\n",
    "rf_patience = 3  # Number of rounds without improvement before stopping\n",
    "\n",
    "# Fit the model\n",
    "rf_halving_grid_search.fit(rf_final_train_features, rf_train_labels)\n",
    "\n",
    "# Access results and implement early stopping manually\n",
    "for idx in range(len(rf_halving_grid_search.cv_results_['mean_test_score'])):\n",
    "    rf_macro_f1_score = rf_halving_grid_search.cv_results_['mean_test_score'][idx]\n",
    "    print(f\"Iteration {idx + 1}: Macro F1 Score = {rf_macro_f1_score:.4f}\")\n",
    "\n",
    "    if rf_macro_f1_score > rf_best_macro_f1_score:\n",
    "        rf_best_macro_f1_score = rf_macro_f1_score\n",
    "        rf_no_improvement_count = 0\n",
    "    else:\n",
    "        rf_no_improvement_count += 1\n",
    "\n",
    "    if rf_no_improvement_count >= rf_patience:\n",
    "        print(\"Early stopping triggered during hyperparameter tuning.\")\n",
    "        break\n",
    "\n",
    "# Best parameters from HalvingGridSearchCV\n",
    "rf_best_rf_classifier = rf_halving_grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {rf_halving_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model\n",
    "rf_predicted_labels = rf_best_rf_classifier.predict(rf_final_test_features)\n",
    "rf_sentiment_accuracy = rf_best_rf_classifier.score(rf_final_test_features, rf_test_labels)\n",
    "rf_macro_f1 = f1_score(rf_test_labels, rf_predicted_labels, average='macro')\n",
    "print(f\"Test Accuracy: {rf_sentiment_accuracy:.2f}\")\n",
    "print(f\"Test Macro F1 Score: {rf_macro_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "rf_sentiment_cm = confusion_matrix(rf_test_labels, rf_predicted_labels)\n",
    "sns.heatmap(rf_sentiment_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf_sentiment_label_mapping.keys(), yticklabels=rf_sentiment_label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(rf_test_labels, rf_predicted_labels, target_names=rf_sentiment_label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Extraction and Preprocessing:\n",
    "\n",
    "Features:\n",
    "\t•\tText Features:\n",
    "\t•\tTF-IDF Representation: Converts textual reviews into numerical feature vectors using Term Frequency-Inverse Document Frequency (TF-IDF). The text is tokenized, vectorized, and limited to 5,000 features.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tApproval Rating: Normalized to the range [0, 1].\n",
    "\t•\tUser Reputation: Normalized by dividing by the maximum value.\n",
    "\t•\tReply Count: Normalized by dividing by the maximum value.\n",
    "\t•\tBest Score: Normalized by dividing by the maximum value.\n",
    "\n",
    "Combined Features:\n",
    "\t•\tText features and numerical features are stacked horizontally into a single feature matrix for training and testing.\n",
    "\n",
    "2. Model Architecture: Random Forest\n",
    "\t•\tRandom Forest consists of an ensemble of decision trees, where each tree is trained on a random subset of data and features.\n",
    "\t•\tKey components:\n",
    "\t•\tBagging (Bootstrap Aggregation):\n",
    "\t•\tEach tree is trained on a randomly sampled subset of the training data (with replacement).\n",
    "\t•\tRandom Feature Selection:\n",
    "\t•\tAt each split in the tree, only a random subset of features is considered.\n",
    "\t•\tVoting:\n",
    "\t•\tFor classification, the final prediction is made by majority vote across all trees.\n",
    "\n",
    "Hyperparameters Tuned:\n",
    "\t•\tn_estimators: The number of decision trees in the forest.\n",
    "\t•\tmax_depth: The maximum depth of each tree.\n",
    "\t•\tmin_samples_split: The minimum number of samples required to split an internal node.\n",
    "\t•\tclass_weight: Weights assigned to each class to handle imbalanced datasets.\n",
    "\n",
    "3. HalvingGridSearchCV: Hyperparameter Tuning\n",
    "\t•\tHalvingGridSearchCV is used to optimize the Random Forest’s hyperparameters based on the macro F1-score.\n",
    "\t•\tHow It Works:\n",
    "\t•\tStarts with a large set of hyperparameter combinations.\n",
    "\t•\tAllocates fewer resources (training data and iterations) to combinations that perform poorly.\n",
    "\t•\tGradually narrows down to the best-performing combinations.\n",
    "\t•\tCustom Macro F1 Scorer:\n",
    "\t•\tOptimizes for the macro F1-score, which gives equal weight to all classes, making it suitable for imbalanced datasets.\n",
    "\n",
    "Early Stopping:\n",
    "\t•\tManual Early Stopping:\n",
    "\t•\tTracks the improvement in macro F1-score across iterations.\n",
    "\t•\tStops hyperparameter tuning after 3 consecutive iterations with no improvement.\n",
    "\n",
    "4. Model Evaluation:\n",
    "Metrics:\n",
    "\t•\tMacro F1-Score:\n",
    "\t•\tComputes the F1-score for each class and averages them, treating all classes equally.\n",
    "\t•\tAccuracy:\n",
    "\t•\tProportion of correctly classified samples.\n",
    "\t•\tClassification Report:\n",
    "\t•\tIncludes precision, recall, and F1-score for each class.\n",
    "\t•\tConfusion Matrix:\n",
    "\t•\tVisualizes the distribution of true vs. predicted labels for all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow Summary:\n",
    "1.\tData Preparation:\n",
    "\t•\tClean and preprocess data (normalize numerical features, extract text features using TF-IDF).\n",
    "\t•\tSplit data into training and testing sets.\n",
    "\n",
    "2.\tModel Initialization:\n",
    "\t•\tRandom Forest Classifier is used as the base model.\n",
    "\t•\tHyperparameter grid specifies the range of values for tuning.\n",
    "\n",
    "3.\tHyperparameter Tuning:\n",
    "\t•\tHalvingGridSearchCV selects the best hyperparameters by progressively eliminating underperforming combinations.\n",
    "\t•\tOptimizes for macro F1-score.\n",
    "\n",
    "4.\tModel Training:\n",
    "\t•\tTrains the best Random Forest model using the selected hyperparameters.\n",
    "\n",
    "5.\tEvaluation:\n",
    "\t•\tTests the model on the test set.\n",
    "\t•\tComputes accuracy, macro F1-score, confusion matrix, and classification report.\n",
    "\n",
    "Explanation of Key Steps:\n",
    "\n",
    "Text Features (TF-IDF):\n",
    "\t•\tText data is converted into numerical vectors using TF-IDF, capturing the importance of words relative to their frequency in the corpus.\n",
    "\n",
    "Numerical Features:\n",
    "\t•\tFeatures like approval rating, user reputation, reply count, and best score are normalized to ensure all features are on a similar scale.\n",
    "\n",
    "Macro F1-Score:\n",
    "\t•\tTreats all classes equally, emphasizing the model’s ability to perform well on minority classes.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\t•\tHalvingGridSearchCV progressively narrows down the best hyperparameters by reducing computational resources for weaker candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Filter missing or invalid rows\n",
    "lstm_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "lstm_df = lstm_df[lstm_df['stars'].between(0, 5)]\n",
    "\n",
    "# Map stars to sentiment categories\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "lstm_df['sentiment'] = lstm_df['stars'].apply(map_star_to_sentiment)\n",
    "sentiment_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "lstm_df['sentiment_encoded'] = lstm_df['sentiment'].map(sentiment_label_mapping)\n",
    "\n",
    "# Extract text, numerical features, and labels\n",
    "lstm_texts = lstm_df['text'].astype(str).tolist()\n",
    "lstm_approval_ratings = lstm_df['approval_rating'].tolist()\n",
    "lstm_user_reputation = lstm_df['user_reputation'].tolist()\n",
    "lstm_reply_count = lstm_df['reply_count'].tolist()\n",
    "lstm_best_score = lstm_df['best_score'].tolist()\n",
    "lstm_labels = lstm_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "lstm_X_train_text, lstm_X_test_text, lstm_X_train_approval, lstm_X_test_approval, \\\n",
    "lstm_X_train_reputation, lstm_X_test_reputation, lstm_X_train_reply, lstm_X_test_reply, \\\n",
    "lstm_X_train_score, lstm_X_test_score, lstm_y_train, lstm_y_test = train_test_split(\n",
    "    lstm_texts, lstm_approval_ratings, lstm_user_reputation, lstm_reply_count, lstm_best_score, lstm_labels,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "lstm_X_train_approval = np.array(lstm_X_train_approval).reshape(-1, 1) / 100.0\n",
    "lstm_X_test_approval = np.array(lstm_X_test_approval).reshape(-1, 1) / 100.0\n",
    "lstm_X_train_reputation = np.array(lstm_X_train_reputation).reshape(-1, 1) / max(lstm_user_reputation)\n",
    "lstm_X_test_reputation = np.array(lstm_X_test_reputation).reshape(-1, 1) / max(lstm_user_reputation)\n",
    "lstm_X_train_reply = np.array(lstm_X_train_reply).reshape(-1, 1) / max(lstm_reply_count)\n",
    "lstm_X_test_reply = np.array(lstm_X_test_reply).reshape(-1, 1) / max(lstm_reply_count)\n",
    "lstm_X_train_score = np.array(lstm_X_train_score).reshape(-1, 1) / max(lstm_best_score)\n",
    "lstm_X_test_score = np.array(lstm_X_test_score).reshape(-1, 1) / max(lstm_best_score)\n",
    "\n",
    "lstm_X_train_numerical = np.hstack((\n",
    "    lstm_X_train_approval, lstm_X_train_reputation, lstm_X_train_reply, lstm_X_train_score\n",
    "))\n",
    "lstm_X_test_numerical = np.hstack((\n",
    "    lstm_X_test_approval, lstm_X_test_reputation, lstm_X_test_reply, lstm_X_test_score\n",
    "))\n",
    "\n",
    "# Step 3: Tokenize the text data\n",
    "lstm_max_words = 5000\n",
    "lstm_max_len = 50\n",
    "\n",
    "lstm_tokenizer = Tokenizer(num_words=lstm_max_words, oov_token='<OOV>')\n",
    "lstm_tokenizer.fit_on_texts(lstm_X_train_text)\n",
    "\n",
    "lstm_X_train_seq = lstm_tokenizer.texts_to_sequences(lstm_X_train_text)\n",
    "lstm_X_test_seq = lstm_tokenizer.texts_to_sequences(lstm_X_test_text)\n",
    "\n",
    "lstm_X_train_padded = pad_sequences(lstm_X_train_seq, maxlen=lstm_max_len, padding='post', truncating='post')\n",
    "lstm_X_test_padded = pad_sequences(lstm_X_test_seq, maxlen=lstm_max_len, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the Bidirectional LSTM Model\n",
    "def build_lstm_model(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    text_input = tf.keras.layers.Input(shape=(lstm_max_len,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=lstm_max_words, output_dim=64)(text_input)\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True))(embedding)\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    \n",
    "    # Numerical input branch\n",
    "    numerical_input = tf.keras.layers.Input(shape=(lstm_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([pooling, numerical_input])\n",
    "    \n",
    "    # Dense layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    output = tf.keras.layers.Dense(len(np.unique(lstm_y_train)), activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 5: Compute Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(lstm_y_train),\n",
    "    y=lstm_y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with class weights\n",
    "best_macro_f1 = 0\n",
    "best_params = {}\n",
    "\n",
    "param_grid = {\n",
    "    'units': [64],  # Test fewer unit sizes\n",
    "    'dropout_rate': [0.3],  # Test fewer dropout rates\n",
    "    'learning_rate': [1e-3]  # Test one learning rate\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing configuration: {params}\")\n",
    "    lstm_model = build_lstm_model(**params)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        [lstm_X_train_padded, lstm_X_train_numerical], np.array(lstm_y_train),\n",
    "        validation_data=([lstm_X_test_padded, lstm_X_test_numerical], np.array(lstm_y_test)),\n",
    "        epochs=5,  # Reduce maximum epochs\n",
    "        batch_size=16,  # Smaller batch size\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights_dict,  # Add class weights here\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate Macro F1 score\n",
    "    lstm_y_pred_probs = lstm_model.predict([lstm_X_test_padded, lstm_X_test_numerical])\n",
    "    lstm_y_pred = lstm_y_pred_probs.argmax(axis=1)\n",
    "    macro_f1 = f1_score(lstm_y_test, lstm_y_pred, average='macro')\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_params = params\n",
    "        best_model = lstm_model\n",
    "\n",
    "# Step 6: Display the Best Configuration\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_macro_f1:.4f}\")\n",
    "\n",
    "# Final Evaluation\n",
    "lstm_y_pred = best_model.predict([lstm_X_test_padded, lstm_X_test_numerical]).argmax(axis=1)\n",
    "\n",
    "# Generate classification report with text labels\n",
    "target_names = list(sentiment_label_mapping.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(lstm_y_test, lstm_y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing\n",
    "\n",
    "Features:\n",
    "\t•\tText Features:\n",
    "\t•\tTokenization: Converts text reviews into sequences of word indices.\n",
    "\t•\tPadding: Pads or truncates sequences to a fixed length (lstm_max_len = 50).\n",
    "\t•\tEmbedding: Encodes each word index into a dense vector representation (embedding).\n",
    "\t•\tNumerical Features:\n",
    "\t•\tIncludes approval ratings, user reputation, reply count, and best score.\n",
    "\t•\tNormalization: Scales numerical features to a range of [0, 1].\n",
    "\n",
    "Labels:\n",
    "\t•\tSentiment Encoding:\n",
    "\t•\tMaps star ratings to sentiment categories (negative, neutral, positive).\n",
    "\t•\tConverts these categories into integer labels (0, 1, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model Architecture\n",
    "\n",
    "The model consists of two input branches: one for textual features and one for numerical features.\n",
    "\n",
    "Text Input Branch:\n",
    "1.\tEmbedding Layer:\n",
    "\t•\tConverts word indices into dense vectors of size 64.\n",
    "\t•\tInput: Tokenized and padded sequences of text.\n",
    "\t•\tOutput: Dense word embeddings for each word in the sequence.\n",
    "\n",
    "2.\tBidirectional LSTM:\n",
    "\t•\tProcesses the text sequence forward and backward to capture both past and future context.\n",
    "\t•\tUnits: 64.\n",
    "\t•\tReturn Sequences: Outputs hidden states for each timestep.\n",
    "\t•\tOutput Shape: (batch_size, max_len, 128).\n",
    "\n",
    "3.\tGlobal Average Pooling:\n",
    "\t•\tReduces the output of the LSTM to a single vector by averaging across timesteps.\n",
    "\t•\tPurpose: Condenses the sequence into a fixed-length vector representation.\n",
    "\n",
    "Numerical Input Branch:\n",
    "\t•\tDirectly accepts the normalized numerical features.\n",
    "\t•\tShape: (batch_size, 4).\n",
    "\n",
    "Combined Features:\n",
    "1.\tConcatenation:\n",
    "\t•\tCombines the textual feature vector (from the LSTM branch) and the numerical features into a single vector.\n",
    "\n",
    "2.\tDense Layer:\n",
    "\t•\tFully connected layer with 64 units and ReLU activation.\n",
    "\t•\tLearns interactions between textual and numerical features.\n",
    "\n",
    "3.\tDropout:\n",
    "\t•\tPrevents overfitting by randomly deactivating 30% of the neurons during training.\n",
    "\n",
    "4.\tOutput Layer:\n",
    "\t•\tFully connected layer with 3 neurons (one for each sentiment class: negative, neutral, positive).\n",
    "\t•\tSoftmax activation outputs probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training Workflow\n",
    "\n",
    "Class Weights:\n",
    "\t•\tComputes weights for each class to handle class imbalances.\n",
    "\t•\tEnsures that minority classes are not ignored during training.\n",
    "\n",
    "Parameter Tuning:\n",
    "\t•\tUses a grid search over:\n",
    "\t•\tunits: Number of LSTM units.\n",
    "\t•\tdropout_rate: Fraction of neurons to deactivate for regularization.\n",
    "\t•\tlearning_rate: Step size for weight updates.\n",
    "\t•\tTracks the macro F1 score to identify the best configuration.\n",
    "\n",
    "Early Stopping:\n",
    "\t•\tMonitors validation loss during training.\n",
    "\t•\tStops training if validation loss does not improve for 2 consecutive epochs.\n",
    "\t•\tRestores the model weights from the epoch with the best validation loss.\n",
    "\n",
    "Evaluation Metrics:\n",
    "1.\tMacro F1 Score:\n",
    "\t•\tBalances precision and recall for each class, treating all classes equally.\n",
    "\t•\tSuitable for imbalanced datasets.\n",
    "2.\tClassification Report:\n",
    "\t•\tProvides precision, recall, and F1-score for each sentiment class.\n",
    "3.\tConfusion Matrix:\n",
    "\t•\tVisualizes true vs. predicted sentiment labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Workflow Steps\n",
    "\t1.\tData Preparation:\n",
    "\t•\tPreprocesses text and numerical features.\n",
    "\t•\tSplits data into training and test sets.\n",
    "\t2.\tModel Building:\n",
    "\t•\tCreates a Bidirectional LSTM-based model with combined inputs.\n",
    "\t•\tUses ReLU for intermediate layers and softmax for the output layer.\n",
    "\t3.\tTraining:\n",
    "\t•\tApplies class weights to handle imbalanced data.\n",
    "\t•\tUses early stopping to prevent overfitting.\n",
    "\t4.\tEvaluation:\n",
    "\t•\tComputes the macro F1 score on the test set.\n",
    "\t•\tGenerates a classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, GlobalMaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df_model2 = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "df_model2 = df_model2[df_model2['stars'].between(0, 5)]\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "df_model2['sentiment'] = df_model2['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df_model2['sentiment_encoded'] = df_model2['sentiment'].map(label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "texts = df_model2['text'].astype(str).tolist()\n",
    "approval_ratings = df_model2['approval_rating'].tolist()\n",
    "user_reputations = df_model2['user_reputation'].tolist()\n",
    "reply_counts = df_model2['reply_count'].tolist()\n",
    "best_scores = df_model2['best_score'].tolist()\n",
    "labels = df_model2['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_text, X_test_text, X_train_rating, X_test_rating, X_train_reputation, X_test_reputation, X_train_reply, X_test_reply, X_train_score, X_test_score, y_train, y_test = train_test_split(\n",
    "    texts, approval_ratings, user_reputations, reply_counts, best_scores, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize and pad text data\n",
    "vocab_size_model2 = 5000\n",
    "max_seq_len_model2 = 100\n",
    "\n",
    "tokenizer_model2 = Tokenizer(num_words=vocab_size_model2)\n",
    "tokenizer_model2.fit_on_texts(texts)\n",
    "\n",
    "X_train_seq_model2 = tokenizer_model2.texts_to_sequences(X_train_text)\n",
    "X_test_seq_model2 = tokenizer_model2.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train_padded_model2 = pad_sequences(X_train_seq_model2, maxlen=max_seq_len_model2, padding='post')\n",
    "X_test_padded_model2 = pad_sequences(X_test_seq_model2, maxlen=max_seq_len_model2, padding='post')\n",
    "\n",
    "# Normalize numerical features\n",
    "X_train_rating_model2 = np.array(X_train_rating).reshape(-1, 1) / 100.0  # Normalize to 0-1 range\n",
    "X_test_rating_model2 = np.array(X_test_rating).reshape(-1, 1) / 100.0\n",
    "X_train_reputation_model2 = np.array(X_train_reputation).reshape(-1, 1) / df_model2['user_reputation'].max()\n",
    "X_test_reputation_model2 = np.array(X_test_reputation).reshape(-1, 1) / df_model2['user_reputation'].max()\n",
    "X_train_reply_model2 = np.array(X_train_reply).reshape(-1, 1) / df_model2['reply_count'].max()\n",
    "X_test_reply_model2 = np.array(X_test_reply).reshape(-1, 1) / df_model2['reply_count'].max()\n",
    "X_train_score_model2 = np.array(X_train_score).reshape(-1, 1) / df_model2['best_score'].max()\n",
    "X_test_score_model2 = np.array(X_test_score).reshape(-1, 1) / df_model2['best_score'].max()\n",
    "\n",
    "# Combine numerical features\n",
    "X_train_numerical_model2 = np.hstack((X_train_rating_model2, X_train_reputation_model2, X_train_reply_model2, X_train_score_model2))\n",
    "X_test_numerical_model2 = np.hstack((X_test_rating_model2, X_test_reputation_model2, X_test_reply_model2, X_test_score_model2))\n",
    "\n",
    "# Define the model with hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    text_input = Input(shape=(max_seq_len_model2,), name=\"text_input\")\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size_model2,\n",
    "        output_dim=hp.Int(\"embedding_dim\", min_value=64, max_value=128, step=64),  # Reduced range\n",
    "        input_length=max_seq_len_model2\n",
    "    )(text_input)\n",
    "    conv = Conv1D(\n",
    "        filters=hp.Int(\"filters\", min_value=32, max_value=64, step=32),  # Reduced range\n",
    "        kernel_size=hp.Choice(\"kernel_size\", values=[3, 5]),  # Fewer options\n",
    "        activation=\"relu\"\n",
    "    )(embedding)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "\n",
    "    # Numerical features input\n",
    "    numerical_input = Input(shape=(4,), name=\"numerical_input\")\n",
    "    combined = Concatenate()([pool, numerical_input])\n",
    "\n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(hp.Int(\"units\", min_value=64, max_value=128, step=64), activation=\"relu\")(combined)  # Reduced range\n",
    "    dropout = Dropout(hp.Float(\"dropout\", min_value=0.1, max_value=0.3, step=0.1))(fc1)  # Reduced range\n",
    "    output = Dense(3, activation=\"softmax\", name=\"output\")(dropout)\n",
    "\n",
    "    model = Model(inputs=[text_input, numerical_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-3, sampling=\"log\")  # Narrow range\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "import tempfile\n",
    "\n",
    "# Use a temporary writable directory for Keras Tuner\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Create the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=5,  # Reduced epochs\n",
    "    factor=4,  # Increased reduction factor to reduce trials\n",
    "    directory=temp_dir,  # Temporary directory for tuner data\n",
    "    project_name=\"cnn_tuning_model2\"\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search with a subset of data\n",
    "subset_size = len(X_train_padded_model2) // 2  # Use 50% of the training data\n",
    "tuner.search(\n",
    "    [X_train_padded_model2[:subset_size], X_train_numerical_model2[:subset_size]],\n",
    "    np.array(y_train[:subset_size]),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]  # Short patience\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search with a subset of data\n",
    "subset_size = len(X_train_padded_model2) // 2  # Use 50% of the training data\n",
    "tuner.search(\n",
    "    [X_train_padded_model2[:subset_size], X_train_numerical_model2[:subset_size]],\n",
    "    np.array(y_train[:subset_size]),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]  # Short patience\n",
    ")\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The optimal embedding dimension is {best_hps.get('embedding_dim')},\n",
    "with {best_hps.get('filters')} filters, kernel size of {best_hps.get('kernel_size')},\n",
    "{best_hps.get('units')} units in the dense layer, dropout of {best_hps.get('dropout')},\n",
    "and learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    [X_train_padded_model2, X_train_numerical_model2],\n",
    "    np.array(y_train),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = best_model.evaluate(\n",
    "    [X_test_padded_model2, X_test_numerical_model2], \n",
    "    np.array(y_test)\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = np.argmax(best_model.predict([X_test_padded_model2, X_test_numerical_model2]), axis=1)\n",
    "\n",
    "# Compute macro F1-score\n",
    "macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"Macro F1-Score: {macro_f1}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing\n",
    "\n",
    "Features:\n",
    "\n",
    "1.\tText Features:\n",
    "\t•\tTokenized into sequences of word indices using a Tokenizer.\n",
    "\t•\tPadded to a fixed length (max_seq_len_model2 = 100) for uniform input dimensions.\n",
    "\t•\tEncoded as dense word vectors using an embedding layer.\n",
    "\n",
    "2.\tNumerical Features:\n",
    "\t•\tIncludes approval ratings, user reputation, reply count, and best score.\n",
    "\t•\tNormalized to the range [0, 1].\n",
    "\n",
    "Labels:\n",
    "\t•\tSentiment Encoding:\n",
    "\t•\tMaps star ratings to sentiment categories (negative, neutral, positive).\n",
    "\t•\tConverts sentiment categories into integer labels (0, 1, 2).\n",
    "\n",
    "Data Splitting:\n",
    "\t•\tThe dataset is split into training and testing sets (80% train, 20% test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Model Architecture\n",
    "\n",
    "The model consists of two input branches: one for textual features and one for numerical features.\n",
    "\n",
    "Text Input Branch:\n",
    "1.\tEmbedding Layer:\n",
    "\t•\tConverts word indices into dense vectors.\n",
    "\t•\tHyperparameter: embedding_dim (searches values between 64 and 128 during tuning).\n",
    "\n",
    "2.\t1D Convolutional Layer:\n",
    "\t•\tApplies convolutional filters over the embedded text to extract local patterns.\n",
    "\t•\tHyperparameters:\n",
    "\t•\tfilters: Number of filters (searches values between 32 and 64).\n",
    "\t•\tkernel_size: Size of the convolutional kernel (searches values [3, 5]).\n",
    "\t•\tOutput Shape: (batch_size, max_seq_len_model2 - kernel_size + 1, filters).\n",
    "\n",
    "3.\tGlobal Max Pooling:\n",
    "\t•\tReduces the feature maps into a fixed-length vector by taking the maximum value across each feature map.\n",
    "\n",
    "Numerical Input Branch:\n",
    "\t•\tReceives the normalized numerical features directly.\n",
    "\t•\tInput Shape: (batch_size, 4) (one feature for each numerical input).\n",
    "\n",
    "Combined Features:\n",
    "1.\tConcatenation:\n",
    "\t•\tCombines the textual and numerical features into a single vector.\n",
    "\n",
    "2.\tFully Connected (Dense) Layer:\n",
    "\t•\tLearns complex interactions between combined features.\n",
    "\t•\tHyperparameters:\n",
    "\t•\tunits: Number of neurons in the dense layer (searches values between 64 and 128).\n",
    "\t•\tActivation: ReLU.\n",
    "\n",
    "3.\tDropout:\n",
    "\t•\tRegularizes the dense layer by randomly deactivating neurons during training.\n",
    "\t•\tHyperparameter: dropout (searches values between 0.1 and 0.3).\n",
    "\n",
    "4.\tOutput Layer:\n",
    "\t•\tFully connected layer with 3 neurons (one for each sentiment class: negative, neutral, positive).\n",
    "\t•\tActivation: Softmax (outputs class probabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training Workflow\n",
    "\n",
    "Early Stopping:\n",
    "\t•\tMonitors validation loss and stops training if no improvement is observed for 2 consecutive epochs.\n",
    "\t•\tRestores the best model weights after stopping.\n",
    "\n",
    "Training Parameters:\n",
    "\t•\tNumber of epochs: 5.\n",
    "\t•\tValidation split: 20% of training data.\n",
    "\t\n",
    "Subset for Hyperparameter Tuning:\n",
    "\t•\tUses 50% of the training data to reduce computational cost during tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics\n",
    "\n",
    "Macro F1 Score:\n",
    "\t•\tBalances precision and recall across all sentiment classes.\n",
    "\t•\tTreats all classes equally, making it suitable for imbalanced datasets.\n",
    "\n",
    "Classification Report:\n",
    "\t•\tProvides precision, recall, and F1-score for each sentiment class.\n",
    "\n",
    "Confusion Matrix:\n",
    "\t•\tVisualizes the true vs. predicted sentiment labels for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow Steps\n",
    "1.\tData Preparation:\n",
    "\t•\tProcesses text and numerical features.\n",
    "\t•\tSplits the dataset into training and testing sets.\n",
    "\n",
    "2.\tHyperparameter Tuning:\n",
    "\t•\tSearches for the best combination of hyperparameters using Keras Tuner.\n",
    "\n",
    "3.\tModel Training:\n",
    "\t•\tTrains the best model on the full training set.\n",
    "\n",
    "4.\tEvaluation:\n",
    "\t•\tComputes macro F1 score, generates a classification report, and plots a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Star Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Filter missing or invalid rows\n",
    "bow_df = df.dropna(subset=['text', 'stars'])\n",
    "bow_df = bow_df[bow_df['stars'].between(0, 5)]  # Ensure stars range is 1-5\n",
    "\n",
    "# Step 2: Preprocess text and labels\n",
    "bow_texts = bow_df['text'].astype(str)\n",
    "bow_labels = bow_df['stars'] - 1  # Convert labels to 0-based indexing\n",
    "\n",
    "# Step 3: Split into train and test sets\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(\n",
    "    bow_texts, bow_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Vectorize the text data using Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=5000)  # Limit to top 5000 words\n",
    "bow_X_train_vec = vectorizer.fit_transform(bow_X_train)\n",
    "bow_X_test_vec = vectorizer.transform(bow_X_test)\n",
    "\n",
    "# Step 5: Train a Logistic Regression model\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "classifier.fit(bow_X_train_vec, bow_y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "bow_y_pred = classifier.predict(bow_X_test_vec)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "f1 = f1_score(bow_y_test, bow_y_pred, average='weighted')\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(bow_y_test, bow_y_pred, target_names=[f\"{i+1}-Star\" for i in range(5)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(bow_y_test, bow_y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f\"{i+1}-Star\" for i in range(5)], yticklabels=[f\"{i+1}-Star\" for i in range(5)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Bag of Words Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and Trigrams by Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Step 1: Ensure you have the necessary NLTK stopwords installed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 2: Define a custom stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Extend with custom stop words and common contractions\n",
    "additional_stop_words = {'39', 've', 'don', 'll', 'didn', 'doesn', 'wasn', 'won', 'like', 'make', 'good', 'recipe', 'followed', 'next time'}\n",
    "stop_words.update(additional_stop_words)\n",
    "\n",
    "# Step 3: Preprocessing function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing punctuation, lowercasing, and filtering stopwords.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
    "    words = text.lower().split()\n",
    "    return \" \".join([word for word in words if word not in stop_words])\n",
    "\n",
    "# Step 4: Clean the text data for sentiment analysis\n",
    "df_model2['cleaned_text'] = df_model2['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Step 5: Function for extracting bigrams and trigrams\n",
    "def get_ngrams(corpus, ngram_range=(2, 3), top_n=20):\n",
    "    \"\"\"\n",
    "    Extract top n bigrams and trigrams from the corpus.\n",
    "\n",
    "    Args:\n",
    "    - corpus (list of str): Text data\n",
    "    - ngram_range (tuple): Range of ngrams to extract (e.g., (2, 3) for bigrams and trigrams)\n",
    "    - top_n (int): Number of top ngrams to return\n",
    "\n",
    "    Returns:\n",
    "    - Counter: Counter object with top ngrams and their frequencies\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    ngram_matrix = vectorizer.fit_transform(corpus)\n",
    "    ngram_counts = ngram_matrix.sum(axis=0).A1\n",
    "    ngram_vocab = vectorizer.get_feature_names_out()\n",
    "    ngram_counter = Counter(dict(zip(ngram_vocab, ngram_counts)))\n",
    "    # Remove 'next time' ngram\n",
    "    ngram_counter = Counter({key: count for key, count in ngram_counter.items() if 'next time' not in key})\n",
    "    return ngram_counter.most_common(top_n)\n",
    "\n",
    "# Step 6: Generate bigrams and trigrams for each sentiment label\n",
    "for sentiment in sorted(df_model2['sentiment'].unique()):\n",
    "    sentiment_texts = df_model2[df_model2['sentiment'] == sentiment]['cleaned_text']\n",
    "    print(f\"\\nTop Bigrams and Trigrams for '{sentiment}' Sentiment:\")\n",
    "    top_ngrams = get_ngrams(sentiment_texts, ngram_range=(2, 3), top_n=10)\n",
    "    for ngram, count in top_ngrams:\n",
    "        print(f\"{ngram}: {count}\")\n",
    "\n",
    "# Step 7 (Optional): Visualize ngrams for a specific sentiment\n",
    "def plot_ngrams(sentiment_label, ngram_range=(2, 3), top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the top ngrams for a specific sentiment.\n",
    "\n",
    "    Args:\n",
    "    - sentiment_label (str): The sentiment label to filter on ('negative', 'neutral', or 'positive')\n",
    "    - ngram_range (tuple): Range of ngrams to extract\n",
    "    - top_n (int): Number of top ngrams to visualize\n",
    "    \"\"\"\n",
    "    sentiment_texts = df_model2[df_model2['sentiment'] == sentiment_label]['cleaned_text']\n",
    "    top_ngrams = get_ngrams(sentiment_texts, ngram_range=ngram_range, top_n=top_n)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    ngrams, counts = zip(*top_ngrams)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(ngrams, counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Ngrams')\n",
    "    plt.title(f'Top {top_n} Bigrams and Trigrams for {sentiment_label.capitalize()} Sentiment')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot for 'positive' sentiment\n",
    "plot_ngrams(sentiment_label='positive', ngram_range=(2, 3), top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display all Recipes List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Get the value counts of 'recipe_name' as a DataFrame\n",
    "recipe_counts = df['recipe_name'].value_counts().reset_index()\n",
    "recipe_counts.columns = ['Recipe Name', 'Count']\n",
    "\n",
    "# Convert the DataFrame to a scrollable HTML element\n",
    "html_table = recipe_counts.to_html(index=False)\n",
    "scrollable_table = f\"\"\"\n",
    "<div style=\"height:400px; overflow:auto; border:1px solid black; padding:10px;\">\n",
    "{html_table}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the scrollable table\n",
    "display(HTML(scrollable_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA & Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Parse 'created_at' column as datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Step 2: Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Step 3: Aggregate data by sentiment proportions (daily)\n",
    "daily_sentiment = df.groupby(pd.Grouper(key='created_at', freq='D'))['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Convert proportions to percentages for better readability\n",
    "daily_sentiment *= 100\n",
    "\n",
    "# Step 4: Plot time series for each sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "for sentiment in daily_sentiment.columns:\n",
    "    plt.plot(daily_sentiment.index, daily_sentiment[sentiment], label=f\"{sentiment.capitalize()} Sentiment\")\n",
    "plt.axvline(daily_sentiment.index[-30], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title('Daily Sentiment Percentages Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for Prophet (example: 'positive' sentiment)\n",
    "prophet_data_positive = daily_sentiment['positive'].reset_index().rename(columns={'created_at': 'ds', 'positive': 'y'})\n",
    "\n",
    "# Step 6: Fit Prophet Model for 'positive' sentiment\n",
    "positive_prophet_model = Prophet()\n",
    "positive_prophet_model.fit(prophet_data_positive)\n",
    "\n",
    "# Step 7: Make future predictions with Prophet\n",
    "positive_future = positive_prophet_model.make_future_dataframe(periods=30)  # Forecast next 30 days\n",
    "positive_forecast = positive_prophet_model.predict(positive_future)\n",
    "\n",
    "# Extract Prophet forecasted values for the next 30 days\n",
    "positive_forecast_next_30 = positive_forecast[['ds', 'yhat']].tail(30)\n",
    "positive_forecast_next_30['yhat'] = positive_forecast_next_30['yhat'].clip(lower=0, upper=100)  # Ensure valid percentages\n",
    "\n",
    "# Step 8: Use SARIMA for time series modeling on 'positive' sentiment\n",
    "# Ensure the DataFrame uses the datetime index\n",
    "daily_sentiment.index = pd.to_datetime(daily_sentiment.index)\n",
    "\n",
    "# Fit SARIMA model for 'positive' sentiment with the corrected index\n",
    "sarima_model_positive = SARIMAX(\n",
    "    daily_sentiment['positive'], \n",
    "    order=(1, 1, 1), \n",
    "    seasonal_order=(1, 1, 1, 7)  # Weekly seasonality\n",
    ")\n",
    "sarima_positive_result = sarima_model_positive.fit(disp=False)\n",
    "\n",
    "# Forecast the next 30 days using SARIMA with proper datetime alignment\n",
    "sarima_forecast_next_30_positive = sarima_positive_result.get_forecast(steps=30)\n",
    "\n",
    "# Use the correct forecasted dates from the index\n",
    "forecast_index = pd.date_range(\n",
    "    start=daily_sentiment.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=30, \n",
    "    freq='D'\n",
    ")\n",
    "sarima_forecast_summary_positive = sarima_forecast_next_30_positive.summary_frame()\n",
    "sarima_forecast_summary_positive.index = forecast_index  # Align dates with the forecast\n",
    "\n",
    "# Ensure the forecasts are clipped to valid percentages\n",
    "sarima_forecast_summary_positive['mean'] = sarima_forecast_summary_positive['mean'].clip(lower=0, upper=100)\n",
    "\n",
    "# Step 9: Plot combined actuals and forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sentiment.index, daily_sentiment['positive'], label='Actual Positive Sentiment (%)', color='blue')\n",
    "plt.plot(positive_forecast_next_30['ds'], positive_forecast_next_30['yhat'], label='Prophet Forecast (%)', linestyle='--', color='green')\n",
    "plt.plot(sarima_forecast_summary_positive.index, sarima_forecast_summary_positive['mean'], label='SARIMA Forecast (%)', linestyle='--', color='orange')\n",
    "plt.axvline(daily_sentiment.index[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title('Positive Sentiment Forecast: Prophet vs SARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Combine Prophet and SARIMA forecasts into a summary table\n",
    "forecast_summary_positive = pd.DataFrame({\n",
    "    'Date': positive_forecast_next_30['ds'],\n",
    "    'Prophet Forecast (%)': positive_forecast_next_30['yhat'],\n",
    "    'SARIMA Forecast (%)': sarima_forecast_summary_positive['mean'].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the summary table\n",
    "print(\"\\nPositive Sentiment Forecast (Next 30 Days):\")\n",
    "print(forecast_summary_positive.to_string(index=False))\n",
    "\n",
    "# Step 11: Add inline commentary\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Prophet predicts an average positive sentiment of {positive_forecast_next_30['yhat'].mean():.2f}% over the next 30 days.\")\n",
    "print(f\"SARIMA predicts an average positive sentiment of {sarima_forecast_summary_positive['mean'].mean():.2f}% over the same period.\")\n",
    "print(\"Both models suggest relatively stable positive sentiment trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses time series analysis to predict the daily proportions of positive sentiment for reviews over the next 30 days, combining:\n",
    "\t1.\tProphet: A forecasting tool that handles seasonality and holidays.\n",
    "\t2.\tSARIMA: A statistical model that accounts for trend, seasonality, and residuals.\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "Key Steps:\n",
    "1.\tConvert Date Column:\n",
    "\t•\tConverts the created_at column to a datetime format to enable time-based grouping and forecasting.\n",
    "\n",
    "2.\tMap Star Ratings to Sentiment:\n",
    "\t•\tMaps star ratings into three sentiment categories:\n",
    "\t•\tNegative: 1–2 stars.\n",
    "\t•\tNeutral: 0 or 3 stars.\n",
    "\t•\tPositive: 4–5 stars.\n",
    "\n",
    "3.\tAggregate Sentiment Proportions:\n",
    "\t•\tCalculates daily proportions of each sentiment category:\n",
    "\t•\tUses value_counts(normalize=True) to compute percentages for each sentiment on a given day.\n",
    "\t•\tFills missing values with 0 to maintain continuity.\n",
    "\n",
    "4.\tConvert Proportions to Percentages:\n",
    "\t•\tMultiplies proportions by 100 for easier interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualization\n",
    "\n",
    "Daily Sentiment Percentages:\n",
    "\t•\tPlots time series of sentiment percentages for negative, neutral, and positive sentiments over time.\n",
    "\t•\tHighlights the forecast start date using a vertical dashed red line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prophet Model\n",
    "\n",
    "Steps:\n",
    "1.\tPrepare Prophet Input:\n",
    "\t•\tSelects the positive sentiment percentages.\n",
    "\t•\tRenames columns to match Prophet’s input format:\n",
    "\t•\tds: Datetime column.\n",
    "\t•\ty: Target variable (positive sentiment percentages).\n",
    "\n",
    "2.\tFit Prophet Model:\n",
    "\t•\tModels the trend and seasonality of positive sentiment percentages over time.\n",
    "\n",
    "3.\tForecast Future Sentiment:\n",
    "\t•\tPredicts the next 30 days of positive sentiment percentages.\n",
    "\t•\tClips the forecast to ensure values stay within [0, 100].\n",
    "\n",
    "Advantages:\n",
    "\t•\tCaptures long-term trends and seasonal effects.\n",
    "\t•\tAutomatically handles missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SARIMA Model\n",
    "\n",
    "Steps:\n",
    "1.\tPrepare Data for SARIMA:\n",
    "\t•\tUses the positive sentiment percentages with the corrected datetime index.\n",
    "\n",
    "2.\tFit SARIMA Model:\n",
    "\t•\tConfigures the SARIMA model with:\n",
    "\t•\tTrend Order: (1, 1, 1) for autoregressive, differencing, and moving average components.\n",
    "\t•\tSeasonal Order: (1, 1, 1, 7) for weekly seasonality.\n",
    "\n",
    "3.\tForecast Future Sentiment:\n",
    "\t•\tPredicts the next 30 days of positive sentiment percentages.\n",
    "\t•\tClips the forecast to valid percentages [0, 100].\n",
    "\n",
    "Advantages:\n",
    "\t•\tProvides a strong statistical baseline for forecasting.\n",
    "\t•\tIncorporates trend and seasonality explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Combined Forecasts\n",
    "\n",
    "Steps:\n",
    "1.\tPlot Actuals and Forecasts:\n",
    "\t•\tPlots:\n",
    "\t•\tActual daily positive sentiment percentages.\n",
    "\t•\tProphet forecast with dashed green lines.\n",
    "\t•\tSARIMA forecast with dashed orange lines.\n",
    "\n",
    "2.\tSummary Table:\n",
    "\t•\tCreates a table combining Prophet and SARIMA forecasts for each day in the next 30 days.\n",
    "\n",
    "3.\tInline Commentary:\n",
    "\t•\tComputes and prints the average predicted positive sentiment percentages for Prophet and SARIMA over the forecast period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Outputs\n",
    "\n",
    "Visualization:\n",
    "\t•\tTime Series Plot:\n",
    "\t•\tShows actual positive sentiment percentages.\n",
    "\t•\tIncludes Prophet and SARIMA forecasts.\n",
    "\t•\tHighlights the start of the forecast period with a red dashed line.\n",
    "\n",
    "Summary Table:\n",
    "\t•\tDisplays daily forecasts for the next 30 days from both Prophet and SARIMA models.\n",
    "\n",
    "Inline Commentary:\n",
    "\t•\tInterprets the forecast results:\n",
    "\t•\tAverage positive sentiment percentage predicted by Prophet.\n",
    "\t•\tAverage positive sentiment percentage predicted by SARIMA.\n",
    "\t•\tObservations about trends and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Sentiment for Specific Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Parse 'created_at' column as datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Step 2: Filter for a specific recipe\n",
    "specific_recipe_name = 'Cheeseburger Soup'  # Change this to your desired recipe\n",
    "recipe_df = df[df['recipe_name'] == specific_recipe_name]\n",
    "\n",
    "# Step 3: Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars == 0:  # Mark 0 stars as neutral\n",
    "        return \"neutral\"\n",
    "    elif stars <= 2:  # 1 and 2 stars as negative\n",
    "        return \"negative\"\n",
    "    elif stars == 3:  # 3 stars as neutral\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars as positive\n",
    "        return \"positive\"\n",
    "\n",
    "recipe_df['sentiment'] = recipe_df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Step 4: Aggregate data by sentiment proportions (daily)\n",
    "daily_sentiment_recipe = recipe_df.groupby(pd.Grouper(key='created_at', freq='D'))['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Convert proportions to percentages for better readability\n",
    "daily_sentiment_recipe *= 100\n",
    "\n",
    "# Step 5: Plot time series for each sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "for sentiment in daily_sentiment_recipe.columns:\n",
    "    plt.plot(daily_sentiment_recipe.index, daily_sentiment_recipe[sentiment], label=f\"{sentiment.capitalize()} Sentiment\")\n",
    "plt.axvline(daily_sentiment_recipe.index[-30], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f'Daily Sentiment Percentages for {specific_recipe_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Prepare data for Prophet (example: 'positive' sentiment)\n",
    "prophet_data_positive_recipe = daily_sentiment_recipe['positive'].reset_index().rename(columns={'created_at': 'ds', 'positive': 'y'})\n",
    "\n",
    "# Step 7: Fit Prophet Model for 'positive' sentiment\n",
    "positive_prophet_model_recipe = Prophet()\n",
    "positive_prophet_model_recipe.fit(prophet_data_positive_recipe)\n",
    "\n",
    "# Step 8: Make future predictions with Prophet\n",
    "positive_future_recipe = positive_prophet_model_recipe.make_future_dataframe(periods=30)  # Forecast next 30 days\n",
    "positive_forecast_recipe = positive_prophet_model_recipe.predict(positive_future_recipe)\n",
    "\n",
    "# Extract Prophet forecasted values for the next 30 days\n",
    "positive_forecast_next_30_recipe = positive_forecast_recipe[['ds', 'yhat']].tail(30)\n",
    "positive_forecast_next_30_recipe['yhat'] = positive_forecast_next_30_recipe['yhat'].clip(lower=0, upper=100)  # Ensure valid percentages\n",
    "\n",
    "# Step 9: Use SARIMA for time series modeling on 'positive' sentiment\n",
    "# Ensure the DataFrame uses the datetime index\n",
    "daily_sentiment_recipe.index = pd.to_datetime(daily_sentiment_recipe.index)\n",
    "\n",
    "# Fit SARIMA model for 'positive' sentiment with the corrected index\n",
    "sarima_model_positive_recipe = SARIMAX(\n",
    "    daily_sentiment_recipe['positive'], \n",
    "    order=(1, 1, 1), \n",
    "    seasonal_order=(1, 1, 1, 7)  # Weekly seasonality\n",
    ")\n",
    "sarima_positive_result_recipe = sarima_model_positive_recipe.fit(disp=False)\n",
    "\n",
    "# Forecast the next 30 days using SARIMA with proper datetime alignment\n",
    "sarima_forecast_next_30_positive_recipe = sarima_positive_result_recipe.get_forecast(steps=30)\n",
    "\n",
    "# Use the correct forecasted dates from the index\n",
    "forecast_index_recipe = pd.date_range(\n",
    "    start=daily_sentiment_recipe.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=30, \n",
    "    freq='D'\n",
    ")\n",
    "sarima_forecast_summary_positive_recipe = sarima_forecast_next_30_positive_recipe.summary_frame()\n",
    "sarima_forecast_summary_positive_recipe.index = forecast_index_recipe  # Align dates with the forecast\n",
    "\n",
    "# Ensure the forecasts are clipped to valid percentages\n",
    "sarima_forecast_summary_positive_recipe['mean'] = sarima_forecast_summary_positive_recipe['mean'].clip(lower=0, upper=100)\n",
    "\n",
    "# Step 10: Plot combined actuals and forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sentiment_recipe.index, daily_sentiment_recipe['positive'], label='Actual Positive Sentiment (%)', color='blue')\n",
    "plt.plot(positive_forecast_next_30_recipe['ds'], positive_forecast_next_30_recipe['yhat'], label='Prophet Forecast (%)', linestyle='--', color='green')\n",
    "plt.plot(sarima_forecast_summary_positive_recipe.index, sarima_forecast_summary_positive_recipe['mean'], label='SARIMA Forecast (%)', linestyle='--', color='orange')\n",
    "plt.axvline(daily_sentiment_recipe.index[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f'Positive Sentiment Forecast for {specific_recipe_name}: Prophet vs SARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Combine Prophet and SARIMA forecasts into a summary table\n",
    "forecast_summary_positive_recipe = pd.DataFrame({\n",
    "    'Date': positive_forecast_next_30_recipe['ds'],\n",
    "    'Prophet Forecast (%)': positive_forecast_next_30_recipe['yhat'],\n",
    "    'SARIMA Forecast (%)': sarima_forecast_summary_positive_recipe['mean'].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the summary table\n",
    "print(f\"\\nPositive Sentiment Forecast for {specific_recipe_name} (Next 30 Days):\")\n",
    "print(forecast_summary_positive_recipe.to_string(index=False))\n",
    "\n",
    "# Step 12: Add inline commentary\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Prophet predicts an average positive sentiment of {positive_forecast_next_30_recipe['yhat'].mean():.2f}% over the next 30 days for {specific_recipe_name}.\")\n",
    "print(f\"SARIMA predicts an average positive sentiment of {sarima_forecast_summary_positive_recipe['mean'].mean():.2f}% over the same period.\")\n",
    "print(\"Both models suggest relatively stable positive sentiment trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Specific Recipe Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Filter dataset for a specific recipe\n",
    "recipe_name_filter = \"Cheeseburger Soup\"\n",
    "filtered_df = df[df['recipe_name'] == recipe_name_filter]\n",
    "\n",
    "# Ensure the filtered DataFrame is not empty\n",
    "if filtered_df.empty:\n",
    "    raise ValueError(f\"No data available for the recipe: {recipe_name_filter}\")\n",
    "\n",
    "# Inspect recent data trends\n",
    "print(f\"\\nRecent Star Ratings for {recipe_name_filter}:\")\n",
    "print(filtered_df[['created_at', 'stars']].sort_values(by='created_at', ascending=False).head(10))\n",
    "\n",
    "# Step 2: Parse 'created_at' column as datetime\n",
    "filtered_df['created_at'] = pd.to_datetime(filtered_df['created_at'], errors='coerce')\n",
    "filtered_df = filtered_df.dropna(subset=['created_at'])  # Drop rows with invalid dates\n",
    "\n",
    "# Step 3: Aggregate data by date (e.g., daily mean of 'stars')\n",
    "recipe_ts = (\n",
    "    filtered_df.groupby(pd.Grouper(key='created_at', freq='D'))['stars']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure there are no missing dates and fill gaps\n",
    "recipe_ts['stars'] = recipe_ts['stars'].interpolate().fillna(0)\n",
    "\n",
    "# Step 4: Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(recipe_ts['created_at'], recipe_ts['stars'], label=f'{recipe_name_filter} Star Ratings', color='blue')\n",
    "plt.title(f'Time Series of Star Ratings for {recipe_name_filter}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for Prophet\n",
    "prophet_data = recipe_ts.rename(columns={'created_at': 'ds', 'stars': 'y'})\n",
    "\n",
    "# Step 6: Fit Prophet model\n",
    "recipe_prophet_model = Prophet()\n",
    "recipe_prophet_model.fit(prophet_data)\n",
    "\n",
    "# Step 7: Make future predictions\n",
    "future = recipe_prophet_model.make_future_dataframe(periods=30)\n",
    "forecast = recipe_prophet_model.predict(future)\n",
    "\n",
    "# Step 8: Rename the forecast columns for clarity (after Prophet operations)\n",
    "forecast_renamed = forecast.rename(columns={\n",
    "    'ds': 'Date',\n",
    "    'yhat': 'Predicted Star Rating',\n",
    "    'yhat_lower': 'Lower Confidence Bound',\n",
    "    'yhat_upper': 'Upper Confidence Bound'\n",
    "})\n",
    "\n",
    "# Step 9: Plot the forecast with forecasted values highlighted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(prophet_data['ds'], prophet_data['y'], label='Historical Data', color='blue')\n",
    "plt.plot(forecast['ds'], forecast['yhat'], label='Forecasted Ratings', color='orange', linestyle='--')\n",
    "plt.fill_between(\n",
    "    forecast['ds'], \n",
    "    forecast['yhat_lower'], \n",
    "    forecast['yhat_upper'], \n",
    "    color='orange', \n",
    "    alpha=0.2, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "plt.axvline(x=prophet_data['ds'].iloc[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f\"Star Ratings Forecast for {recipe_name_filter}\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Seasonal decomposition (keep 'ds' column for Prophet compatibility)\n",
    "fig2 = recipe_prophet_model.plot_components(forecast)\n",
    "plt.show()\n",
    "\n",
    "clear_forecasted_ratings = forecast_renamed[['Date', 'Predicted Star Rating', 'Lower Confidence Bound', 'Upper Confidence Bound']].tail(30)\n",
    "print(\"\\nForecasted Ratings\")\n",
    "print(clear_forecasted_ratings.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
