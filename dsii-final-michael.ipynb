{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. recipe name: {name of the recipe the comment was posted on}\n",
    "2. recipe number: {placement of the recipe on the top 100 recipes list}\n",
    "3. recipe code: {unique id of the recipe used by the site}\n",
    "4. comment id: {unique id of the comment}\n",
    "5. user id: {unique id of the user who left the comment}\n",
    "6. user name: {name of the user}\n",
    "7. user reputation: {internal score of the site, roughly quantifying the past behavior of the user}\n",
    "8. create at: {time at which the comment was posted as a Unix timestamp}\n",
    "9. reply count: {number of replies to the comment}\n",
    "10. thumbs up: {number of up-votes the comment has received}\n",
    "11. thumbs down: {number of down-votes the comment has received}\n",
    "12. stars: {the score on a 1 to 5 scale that the user gave to the recipe. A score of 0 means that no score was given}\n",
    "13. best score: {score of the comment, likely used by the site the help determine the order in the comments that appear in}\n",
    "14. text: {the text content of the comment}\n",
    "\n",
    "unnamed 0: # of times that the recipe was reviewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/mn42899/data_scienceii/refs/heads/main/Recipe%20Reviews%20and%20User%20Feedback%20Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'created_at' column from UNIX timestamp to a readable datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], unit='s')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the approval rating as a percentage\n",
    "df['approval_rating'] = (df['thumbs_up'] / (df['thumbs_up'] + df['thumbs_down'])) * 100\n",
    "\n",
    "# Replace NaN values (e.g., when thumbs_up and thumbs_down are both 0) with 0\n",
    "df['approval_rating'] = df['approval_rating'].fillna(0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of reviews for the specific recipe\n",
    "specific_recipe = \"Mamaw Emily’s Strawberry Cake\"\n",
    "\n",
    "# Ensure the recipe_name column exists and count the entries\n",
    "if 'recipe_name' in df.columns:\n",
    "    recipe_count = df[df['recipe_name'] == specific_recipe].shape[0]\n",
    "    print(f\"Number of reviews for '{specific_recipe}': {recipe_count}\")\n",
    "else:\n",
    "    print(\"The 'recipe_name' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicate Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by recipe_name and count duplicates in the text column\n",
    "duplicates_per_recipe = (\n",
    "    df[df.duplicated(subset=['recipe_name', 'text','user_name' ], keep=False)]  # Filter duplicates\n",
    "    .groupby('recipe_name')\n",
    "    .size()\n",
    "    .reset_index(name='duplicate_count')  # Rename the count column\n",
    ")\n",
    "\n",
    "# Display the counts of duplicates per recipe\n",
    "print(duplicates_per_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on recipe_name and text, keeping the first occurrence\n",
    "df.drop_duplicates(subset=['recipe_name', 'text'], keep='first', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Ensure all values in the 'text' column are strings\n",
    "df['text'] = df['text'].fillna('').astype(str)\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function for text\n",
    "def preprocess_text(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Join tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing directly to the 'text' column\n",
    "df['text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "\n",
    "# Optional: Print the number of rows to verify\n",
    "print(f\"Number of rows after dropping null values: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted histogram for 'best_score' with more bins\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['best_score'], bins=50, edgecolor='black')  # Increased bins to 50\n",
    "plt.title('Histogram of Best Scores (Adjusted Bin Size)', fontsize=16)\n",
    "plt.xlabel('Best Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the minimum and maximum dates\n",
    "min_date = df['created_at'].min()\n",
    "max_date = df['created_at'].max()\n",
    "\n",
    "print(f\"Minimum date: {min_date}\")\n",
    "print(f\"Maximum date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by month and count reviews\n",
    "df['month'] = df['created_at'].dt.to_period('M')\n",
    "monthly_review_distribution = df.groupby('month').size()\n",
    "\n",
    "# Plot the monthly distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_review_distribution.index.astype(str), monthly_review_distribution.values, marker='o')\n",
    "plt.title('Review Distribution Over Time (Monthly)', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Number of Reviews', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='both', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rep = df['user_reputation'].min()\n",
    "max_rep = df['user_reputation'].max()\n",
    "\n",
    "print(f\"Minimum Rep: {min_rep}\")\n",
    "print(f\"Maximum Rep: {max_rep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['user_reputation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram for 'user_reputation'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['user_reputation'], bins=100, edgecolor='black')\n",
    "plt.title('User Reputation Distribution', fontsize=16)\n",
    "plt.xlabel('User Reputation', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by recipe_number and compute summary statistics\n",
    "summary_stats = df.groupby('recipe_number').agg(\n",
    "    total_comments=('comment_id', 'count'),       # Count of comments\n",
    "    avg_user_reputation=('user_reputation', 'mean'),  # Average user reputation\n",
    "    avg_reply_count=('reply_count', 'mean'),     # Average reply count\n",
    "    avg_thumbs_up=('thumbs_up', 'mean'),         # Average thumbs up\n",
    "    avg_thumbs_down=('thumbs_down', 'mean'),     # Average thumbs down\n",
    "    avg_star_rating=('stars', 'mean'),           # Average star rating\n",
    "    avg_best_score=('best_score', 'mean')        # Average best score\n",
    ").reset_index()\n",
    "\n",
    "# Display the results\n",
    "summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a ratio column for thumbs up to thumbs down\n",
    "summary_stats['thumbs_up_to_down_ratio'] = summary_stats['avg_thumbs_up'] / summary_stats['avg_thumbs_down']\n",
    "\n",
    "# Handle cases where thumbs down is 0 to avoid division by zero\n",
    "summary_stats['thumbs_up_to_down_ratio'] = summary_stats['thumbs_up_to_down_ratio'].replace([float('inf'), -float('inf')], None).fillna(0)\n",
    "\n",
    "# Display the updated summary table\n",
    "summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recipes = df.groupby(['recipe_number', 'recipe_name']).size().reset_index(name='total_comments')\n",
    "top10 = top_recipes.sort_values(by='recipe_number', ascending=True)\n",
    "\n",
    "top10.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_volume = df.groupby(df['created_at'].dt.to_period('M')).size().reset_index(name='comment_count')\n",
    "\n",
    "plt.plot(comment_volume['created_at'].dt.to_timestamp(), comment_volume['comment_count'])\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Comment Volume over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of comments occur at 10/11 am (Before lunch time to prep the recipe???)\n",
    "\n",
    "df['hour'] = df['created_at'].dt.hour\n",
    "\n",
    "hourly_distribution = df.groupby('hour').size().reset_index(name='comment_count')\n",
    "\n",
    "#Plot the distribution\n",
    "plt.bar(hourly_distribution['hour'], hourly_distribution['comment_count'])\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Distribution of Comments by Time of Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the comments were happening in June\n",
    "\n",
    "# Group by recipe_number and week\n",
    "recipe_timeline = df.groupby([df['created_at'].dt.to_period('W'), 'recipe_number']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot the activity of top recipes\n",
    "recipe_timeline.plot(figsize=(10, 6))\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Number of Comments')\n",
    "plt.title('Activity of Top Recipes Over Time')\n",
    "plt.legend(title='Recipe Number', bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by day and calculate averages\n",
    "metrics_trend = df.groupby(df['created_at'].dt.to_period('D'))[['reply_count', 'thumbs_up', 'thumbs_down']].mean().reset_index()\n",
    "\n",
    "# Plot each metric over time\n",
    "for metric in ['reply_count', 'thumbs_up', 'thumbs_down']:\n",
    "    plt.plot(metrics_trend['created_at'].dt.to_timestamp(), metrics_trend[metric], label=metric)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Average Metric')\n",
    "plt.title('Metrics Trend Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataset by 'thumbs_up' in descending order\n",
    "top_thumbs_up_comments = df.sort_values(by='thumbs_up', ascending=False)\n",
    "\n",
    "# Select relevant columns to display (including recipe_number and the full text comment)\n",
    "top_comments = top_thumbs_up_comments[['recipe_number', 'text', 'thumbs_up']].head(30)\n",
    "\n",
    "# Display the top comments with the highest thumbs up\n",
    "top_comments.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the ratio of 5 stars compared to other stars for each recipe\n",
    "\n",
    "# Count total ratings for each recipe\n",
    "total_counts = df.groupby('recipe_number').size().reset_index(name='total_ratings')\n",
    "\n",
    "# Count 5-star ratings for each recipe\n",
    "five_star_counts = df[df['stars'] == 5].groupby('recipe_number').size().reset_index(name='five_star_count')\n",
    "\n",
    "# Merge total ratings and 5-star ratings\n",
    "merged_counts = pd.merge(total_counts, five_star_counts, on='recipe_number', how='left')\n",
    "\n",
    "# Fill NaN values in five_star_count (for recipes without any 5-star ratings) with 0\n",
    "merged_counts['five_star_count'] = merged_counts['five_star_count'].fillna(0)\n",
    "\n",
    "# Calculate the ratio of 5-star ratings to total ratings\n",
    "merged_counts['five_star_ratio'] = merged_counts['five_star_count'] / merged_counts['total_ratings']\n",
    "\n",
    "# Display the results\n",
    "print(\"Five-Star Ratio for Each Recipe:\")\n",
    "print(merged_counts[['recipe_number', 'five_star_ratio']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical features and textual data provide complementary information that improves the sentiment classification (negative, neutral, or positive). Here’s how they interact and affect the sentiment analysis:\n",
    "\n",
    "1. Textual Data (TF-IDF)\n",
    "\n",
    "Textual data captures the semantic content of reviews. Words or phrases often carry direct sentiment information. For example:\n",
    "\t•\tWords like “bad,” “terrible,” or “awful” indicate negative sentiment.\n",
    "\t•\tWords like “okay” or “average” suggest neutral sentiment.\n",
    "\t•\tWords like “excellent,” “amazing,” or “perfect” imply positive sentiment.\n",
    "\n",
    "By converting this data into TF-IDF vectors, the model learns the importance of specific words in determining the sentiment:\n",
    "\t•\tHigh TF-IDF scores for positive words (e.g., “great,” “love”) lead to higher chances of predicting the review as positive.\n",
    "\t•\tSimilarly, high TF-IDF scores for negative words (e.g., “poor,” “disappointing”) guide the model toward a negative classification.\n",
    "\n",
    "\n",
    "2. Numerical Features\n",
    "\n",
    "Numerical features like approval_rating, user_reputation, reply_count, and best_score provide additional context about the review or reviewer that can influence sentiment prediction:\n",
    "\n",
    "(a) Approval Rating (approval_rating):\n",
    "\t•\tA higher approval rating could correlate with a more positive sentiment.\n",
    "\t•\tExample: A user who consistently receives high approval ratings for their reviews is likely to write more positive reviews overall.\n",
    "\n",
    "(b) User Reputation (user_reputation):\n",
    "\t•\tA reviewer with high reputation might have a more critical tone, even in neutral or positive reviews.\n",
    "\t•\tExample: A high-reputation user might be more likely to give nuanced feedback, which could alter the sentiment classification.\n",
    "\n",
    "(c) Reply Count (reply_count):\n",
    "\t•\tReviews with a high reply count might generate stronger sentiments (either positive or negative), as they are more likely to elicit reactions.\n",
    "\t•\tExample: A review with many replies could indicate a controversial or strongly worded opinion.\n",
    "\n",
    "(d) Best Score (best_score):\n",
    "\t•\tA high best_score might indicate an exceptional review, likely aligning with positive sentiments.\n",
    "\t•\tExample: Reviews with high scores are more likely to be rated positively by other users, implying a positive tone.\n",
    "\n",
    "\n",
    "3. Effect on the 3 Sentiment Classes\n",
    "\n",
    "When combined, the numerical features and textual data affect the classification as follows:\n",
    "\n",
    "Positive Sentiment (4–5 stars)\n",
    "\t•\tText: Words like “excellent,” “love,” or “fantastic” increase the probability of predicting a positive sentiment.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tHigh approval_rating reinforces positivity.\n",
    "\t•\tHigh reply_count or best_score adds confidence in the prediction.\n",
    "\n",
    "Neutral Sentiment (3 stars)\n",
    "\t•\tText: Words like “okay,” “average,” or “fine” suggest neutrality.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tModerate approval_rating or user_reputation can help identify neutral reviews.\n",
    "\t•\tLow reply_count might indicate less polarizing content.\n",
    "\n",
    "Negative Sentiment (0–2 stars)\n",
    "\t•\tText: Words like “terrible,” “disappointing,” or “poor” increase the likelihood of negative sentiment classification.\n",
    "\t•\tNumerical Features:\n",
    "\t•\tLow approval_rating and low best_score further emphasize negative sentiment.\n",
    "\n",
    "4. How Combined Features Improve Classification\n",
    "\n",
    "The combination of textual and numerical features creates a richer representation of the reviews:\n",
    "\t•\tTextual Data provides context and sentiment-rich signals.\n",
    "\t•\tNumerical Features add contextual metadata that may influence sentiment indirectly.\n",
    "\n",
    "Example:\n",
    "\n",
    "A review says, “The product is okay but arrived late.”\n",
    "\t•\tText Analysis:\n",
    "\t•\tWords like “okay” indicate neutrality, but “arrived late” leans toward negative.\n",
    "\t•\tNumerical Data:\n",
    "\t•\tIf the approval_rating is low and reply_count is high, the classifier might predict a negative sentiment, despite the text being borderline neutral.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset (replace with your actual dataset if needed)\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])  # Ensure no missing values\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels as integers\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Preprocess text, numerical features, and labels\n",
    "texts = df['text'].astype(str).tolist()\n",
    "approval_ratings = df['approval_rating'].tolist()\n",
    "user_reputations = df['user_reputation'].tolist()\n",
    "reply_counts = df['reply_count'].tolist()\n",
    "best_scores = df['best_score'].tolist()\n",
    "labels = df['sentiment_encoded'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data preprocessing (modify as per your dataset)\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating'])  # Ensure no missing values\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment_classifier'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels as integers\n",
    "label_mapping_classifier = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded_classifier'] = df['sentiment_classifier'].map(label_mapping_classifier)\n",
    "\n",
    "# Split dataset\n",
    "X_text_classifier = df['text'].astype(str)\n",
    "X_rating_classifier = df['approval_rating']\n",
    "y_classifier = df['sentiment_encoded_classifier']\n",
    "\n",
    "# Split data for training and testing\n",
    "X_train_text_classifier, X_test_text_classifier, X_train_rating_classifier, X_test_rating_classifier, y_train_classifier, y_test_classifier = train_test_split(\n",
    "    X_text_classifier, X_rating_classifier, y_classifier, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF Vectorizer for text\n",
    "tfidf_vectorizer_classifier = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf_classifier = tfidf_vectorizer_classifier.fit_transform(X_train_text_classifier).toarray()\n",
    "X_test_tfidf_classifier = tfidf_vectorizer_classifier.transform(X_test_text_classifier).toarray()\n",
    "\n",
    "# Normalize approval ratings\n",
    "X_train_rating_classifier = np.array(X_train_rating_classifier).reshape(-1, 1) / 100.0\n",
    "X_test_rating_classifier = np.array(X_test_rating_classifier).reshape(-1, 1) / 100.0\n",
    "\n",
    "# Model-Specific Features\n",
    "models_classifier = {\n",
    "    \"Random Forest\": {\n",
    "        \"features_train\": np.hstack((X_train_tfidf_classifier, X_train_rating_classifier)),\n",
    "        \"features_test\": np.hstack((X_test_tfidf_classifier, X_test_rating_classifier)),\n",
    "        \"model\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"features_train\": X_train_tfidf_classifier,  # Only text features\n",
    "        \"features_test\": X_test_tfidf_classifier,\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "    },\n",
    "    \"Support Vector Machine (SVM)\": {\n",
    "        \"features_train\": X_train_rating_classifier,  # Only approval rating\n",
    "        \"features_test\": X_test_rating_classifier,\n",
    "        \"model\": SVC(kernel='linear', random_state=42)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and Evaluate Models\n",
    "results_classifier = {}\n",
    "for model_name, model_info in models_classifier.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model_classifier = model_info[\"model\"]\n",
    "    X_train_classifier = model_info[\"features_train\"]\n",
    "    X_test_classifier = model_info[\"features_test\"]\n",
    "\n",
    "    # Train the model\n",
    "    model_classifier.fit(X_train_classifier, y_train_classifier)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_classifier = model_classifier.predict(X_test_classifier)\n",
    "    accuracy_classifier = model_classifier.score(X_test_classifier, y_test_classifier)\n",
    "    results_classifier[model_name] = accuracy_classifier\n",
    "\n",
    "    print(f\"{model_name} Accuracy: {accuracy_classifier}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_classifier, y_pred_classifier, target_names=label_mapping_classifier.keys()))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_classifier = confusion_matrix(y_test_classifier, y_pred_classifier)\n",
    "    sns.heatmap(cm_classifier, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping_classifier.keys(), yticklabels=label_mapping_classifier.keys())\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "# Summary of Model Accuracies\n",
    "print(\"\\nModel F1-Score Summary:\")\n",
    "for model_name, f1 in results_classifier.items():\n",
    "    print(f\"{model_name}: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom Callback to Monitor F1 Score\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, patience=2):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y_true = self.validation_data\n",
    "        val_y_pred_probs = self.model.predict(val_x)\n",
    "        val_y_pred = np.argmax(val_y_pred_probs, axis=1)\n",
    "        current_f1 = f1_score(val_y_true, val_y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Macro F1 Score = {current_f1:.4f}\")\n",
    "        \n",
    "        # Save the best weights if F1 improves\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(\"Early stopping triggered due to no improvement in Macro F1 Score.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "bidir_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "bidir_df = bidir_df[bidir_df['stars'].between(0, 5)]\n",
    "\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "bidir_df['sentiment'] = bidir_df['stars'].apply(map_star_to_sentiment)\n",
    "bidir_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "bidir_df['sentiment_encoded'] = bidir_df['sentiment'].map(bidir_label_mapping)\n",
    "\n",
    "bidir_texts = bidir_df['text'].astype(str).tolist()\n",
    "bidir_labels = bidir_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Additional numerical features\n",
    "bidir_approval = bidir_df['approval_rating'].tolist()\n",
    "bidir_reputation = bidir_df['user_reputation'].tolist()\n",
    "bidir_reply = bidir_df['reply_count'].tolist()\n",
    "bidir_score = bidir_df['best_score'].tolist()\n",
    "\n",
    "# Split data\n",
    "bidir_X_train_text, bidir_X_test_text, bidir_y_train, bidir_y_test, bidir_X_train_approval, bidir_X_test_approval, \\\n",
    "bidir_X_train_reputation, bidir_X_test_reputation, bidir_X_train_reply, bidir_X_test_reply, \\\n",
    "bidir_X_train_score, bidir_X_test_score = train_test_split(\n",
    "    bidir_texts, bidir_labels, bidir_approval, bidir_reputation, bidir_reply, bidir_score, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "bidir_X_train_approval = np.array(bidir_X_train_approval).reshape(-1, 1) / 100.0\n",
    "bidir_X_test_approval = np.array(bidir_X_test_approval).reshape(-1, 1) / 100.0\n",
    "bidir_X_train_reputation = np.array(bidir_X_train_reputation).reshape(-1, 1) / max(bidir_reputation)\n",
    "bidir_X_test_reputation = np.array(bidir_X_test_reputation).reshape(-1, 1) / max(bidir_reputation)\n",
    "bidir_X_train_reply = np.array(bidir_X_train_reply).reshape(-1, 1) / max(bidir_reply)\n",
    "bidir_X_test_reply = np.array(bidir_X_test_reply).reshape(-1, 1) / max(bidir_reply)\n",
    "bidir_X_train_score = np.array(bidir_X_train_score).reshape(-1, 1) / max(bidir_score)\n",
    "bidir_X_test_score = np.array(bidir_X_test_score).reshape(-1, 1) / max(bidir_score)\n",
    "\n",
    "bidir_X_train_numerical = np.hstack((\n",
    "    bidir_X_train_approval, bidir_X_train_reputation, bidir_X_train_reply, bidir_X_train_score\n",
    "))\n",
    "bidir_X_test_numerical = np.hstack((\n",
    "    bidir_X_test_approval, bidir_X_test_reputation, bidir_X_test_reply, bidir_X_test_score\n",
    "))\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "bidir_max_vocab_size = 5000\n",
    "bidir_max_sequence_length = 100\n",
    "\n",
    "bidir_tokenizer = Tokenizer(num_words=bidir_max_vocab_size, oov_token=\"<OOV>\")\n",
    "bidir_tokenizer.fit_on_texts(bidir_X_train_text)\n",
    "\n",
    "bidir_X_train_seq = bidir_tokenizer.texts_to_sequences(bidir_X_train_text)\n",
    "bidir_X_test_seq = bidir_tokenizer.texts_to_sequences(bidir_X_test_text)\n",
    "\n",
    "bidir_X_train_padded = pad_sequences(bidir_X_train_seq, maxlen=bidir_max_sequence_length, padding='post', truncating='post')\n",
    "bidir_X_test_padded = pad_sequences(bidir_X_test_seq, maxlen=bidir_max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the BiRNN model\n",
    "def build_bidir_model_with_numerical(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    # Text input and embedding\n",
    "    text_input = tf.keras.layers.Input(shape=(bidir_max_sequence_length,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=bidir_max_vocab_size, output_dim=64)(text_input)\n",
    "    birnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=False))(embedding)\n",
    "    \n",
    "    # Numerical features input\n",
    "    numerical_input = tf.keras.layers.Input(shape=(bidir_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([birnn, numerical_input])\n",
    "    \n",
    "    # Dense and output layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    outputs = tf.keras.layers.Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train and evaluate with hyperparameter tuning\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "units_list = [64]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "for units in units_list:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Testing configuration: units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "            \n",
    "            bidir_model = build_bidir_model_with_numerical(units, dropout_rate, learning_rate)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(\n",
    "                [bidir_X_test_padded, bidir_X_test_numerical], bidir_y_test\n",
    "            ), patience=2)\n",
    "            \n",
    "            history = bidir_model.fit(\n",
    "                [bidir_X_train_padded, bidir_X_train_numerical], np.array(bidir_y_train),\n",
    "                validation_data=(\n",
    "                    [bidir_X_test_padded, bidir_X_test_numerical], np.array(bidir_y_test)\n",
    "                ),\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                callbacks=[f1_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            bidir_y_pred_probs = bidir_model.predict([bidir_X_test_padded, bidir_X_test_numerical])\n",
    "            bidir_y_pred = np.argmax(bidir_y_pred_probs, axis=1)\n",
    "            macro_f1 = f1_score(bidir_y_test, bidir_y_pred, average='macro')\n",
    "            \n",
    "            if macro_f1 > best_f1_score:\n",
    "                best_f1_score = macro_f1\n",
    "                best_params = {\n",
    "                    'units': units,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "                best_model = bidir_model\n",
    "\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "bidir_y_pred = np.argmax(best_model.predict([bidir_X_test_padded, bidir_X_test_numerical]), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(bidir_y_test, bidir_y_pred))\n",
    "\n",
    "cm = confusion_matrix(bidir_y_test, bidir_y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AttBiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Attention Layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"attention_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.dot(x, self.W)\n",
    "        e = tf.keras.backend.squeeze(e, axis=-1)\n",
    "        alpha = tf.keras.backend.softmax(e)\n",
    "        alpha = tf.keras.backend.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = tf.keras.backend.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# Custom Callback to Monitor F1 Score\n",
    "class F1ScoreCallback(Callback):\n",
    "    def __init__(self, validation_data, patience=2):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y_true = self.validation_data\n",
    "        val_y_pred_probs = self.model.predict(val_x)\n",
    "        val_y_pred = np.argmax(val_y_pred_probs, axis=1)\n",
    "        current_f1 = f1_score(val_y_true, val_y_pred, average='macro')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}: Macro F1 Score = {current_f1:.4f}\")\n",
    "        \n",
    "        # Save the best weights if F1 improves\n",
    "        if current_f1 > self.best_f1:\n",
    "            self.best_f1 = current_f1\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                print(\"Early stopping triggered due to no improvement in Macro F1 Score.\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Step 1: Preprocess the dataset\n",
    "attbirnn_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "attbirnn_df = attbirnn_df[attbirnn_df['stars'].between(0, 5)]\n",
    "\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "attbirnn_df['sentiment'] = attbirnn_df['stars'].apply(map_star_to_sentiment)\n",
    "attbirnn_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "attbirnn_df['sentiment_encoded'] = attbirnn_df['sentiment'].map(attbirnn_label_mapping)\n",
    "\n",
    "attbirnn_texts = attbirnn_df['text'].astype(str).tolist()\n",
    "attbirnn_labels = attbirnn_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Additional numerical features\n",
    "attbirnn_approval = attbirnn_df['approval_rating'].tolist()\n",
    "attbirnn_reputation = attbirnn_df['user_reputation'].tolist()\n",
    "attbirnn_reply = attbirnn_df['reply_count'].tolist()\n",
    "attbirnn_score = attbirnn_df['best_score'].tolist()\n",
    "\n",
    "# Split data\n",
    "attbirnn_X_train_text, attbirnn_X_test_text, attbirnn_y_train, attbirnn_y_test, attbirnn_X_train_approval, attbirnn_X_test_approval, \\\n",
    "attbirnn_X_train_reputation, attbirnn_X_test_reputation, attbirnn_X_train_reply, attbirnn_X_test_reply, \\\n",
    "attbirnn_X_train_score, attbirnn_X_test_score = train_test_split(\n",
    "    attbirnn_texts, attbirnn_labels, attbirnn_approval, attbirnn_reputation, attbirnn_reply, attbirnn_score, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "attbirnn_X_train_approval = np.array(attbirnn_X_train_approval).reshape(-1, 1) / 100.0\n",
    "attbirnn_X_test_approval = np.array(attbirnn_X_test_approval).reshape(-1, 1) / 100.0\n",
    "attbirnn_X_train_reputation = np.array(attbirnn_X_train_reputation).reshape(-1, 1) / max(attbirnn_reputation)\n",
    "attbirnn_X_test_reputation = np.array(attbirnn_X_test_reputation).reshape(-1, 1) / max(attbirnn_reputation)\n",
    "attbirnn_X_train_reply = np.array(attbirnn_X_train_reply).reshape(-1, 1) / max(attbirnn_reply)\n",
    "attbirnn_X_test_reply = np.array(attbirnn_X_test_reply).reshape(-1, 1) / max(attbirnn_reply)\n",
    "attbirnn_X_train_score = np.array(attbirnn_X_train_score).reshape(-1, 1) / max(attbirnn_score)\n",
    "attbirnn_X_test_score = np.array(attbirnn_X_test_score).reshape(-1, 1) / max(attbirnn_score)\n",
    "\n",
    "attbirnn_X_train_numerical = np.hstack((\n",
    "    attbirnn_X_train_approval, attbirnn_X_train_reputation, attbirnn_X_train_reply, attbirnn_X_train_score\n",
    "))\n",
    "attbirnn_X_test_numerical = np.hstack((\n",
    "    attbirnn_X_test_approval, attbirnn_X_test_reputation, attbirnn_X_test_reply, attbirnn_X_test_score\n",
    "))\n",
    "\n",
    "# Tokenize and pad the text data\n",
    "attbirnn_max_vocab_size = 5000\n",
    "attbirnn_max_sequence_length = 100\n",
    "\n",
    "attbirnn_tokenizer = Tokenizer(num_words=attbirnn_max_vocab_size, oov_token=\"<OOV>\")\n",
    "attbirnn_tokenizer.fit_on_texts(attbirnn_X_train_text)\n",
    "\n",
    "attbirnn_X_train_seq = attbirnn_tokenizer.texts_to_sequences(attbirnn_X_train_text)\n",
    "attbirnn_X_test_seq = attbirnn_tokenizer.texts_to_sequences(attbirnn_X_test_text)\n",
    "\n",
    "attbirnn_X_train_padded = pad_sequences(attbirnn_X_train_seq, maxlen=attbirnn_max_sequence_length, padding='post', truncating='post')\n",
    "attbirnn_X_test_padded = pad_sequences(attbirnn_X_test_seq, maxlen=attbirnn_max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the AttBiRNN model\n",
    "def build_attbirnn_model_with_numerical(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    # Text input and embedding\n",
    "    text_input = tf.keras.layers.Input(shape=(attbirnn_max_sequence_length,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=attbirnn_max_vocab_size, output_dim=64)(text_input)\n",
    "    birnn = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True))(embedding)\n",
    "    attention = Attention()(birnn)\n",
    "    \n",
    "    # Numerical features input\n",
    "    numerical_input = tf.keras.layers.Input(shape=(attbirnn_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([attention, numerical_input])\n",
    "    \n",
    "    # Dense and output layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    outputs = tf.keras.layers.Dense(5, activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 5: Train and evaluate with hyperparameter tuning\n",
    "best_f1_score = 0\n",
    "best_params = {}\n",
    "\n",
    "units_list = [64]\n",
    "dropout_rates = [0.3, 0.5]\n",
    "learning_rates = [1e-3]\n",
    "\n",
    "for units in units_list:\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Testing configuration: units={units}, dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "            \n",
    "            attbirnn_model = build_attbirnn_model_with_numerical(units, dropout_rate, learning_rate)\n",
    "            \n",
    "            f1_callback = F1ScoreCallback(validation_data=(\n",
    "                [attbirnn_X_test_padded, attbirnn_X_test_numerical], attbirnn_y_test\n",
    "            ), patience=2)\n",
    "            \n",
    "            history = attbirnn_model.fit(\n",
    "                [attbirnn_X_train_padded, attbirnn_X_train_numerical], np.array(attbirnn_y_train),\n",
    "                validation_data=(\n",
    "                    [attbirnn_X_test_padded, attbirnn_X_test_numerical], np.array(attbirnn_y_test)\n",
    "                ),\n",
    "                epochs=10,\n",
    "                batch_size=32,\n",
    "                callbacks=[f1_callback],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            attbirnn_y_pred_probs = attbirnn_model.predict([attbirnn_X_test_padded, attbirnn_X_test_numerical])\n",
    "            attbirnn_y_pred = np.argmax(attbirnn_y_pred_probs, axis=1)\n",
    "            macro_f1 = f1_score(attbirnn_y_test, attbirnn_y_pred, average='macro')\n",
    "            \n",
    "            if macro_f1 > best_f1_score:\n",
    "                best_f1_score = macro_f1\n",
    "                best_params = {\n",
    "                    'units': units,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': learning_rate\n",
    "                }\n",
    "                best_model = attbirnn_model\n",
    "\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_f1_score:.4f}\")\n",
    "\n",
    "attbirnn_y_pred = np.argmax(best_model.predict([attbirnn_X_test_padded, attbirnn_X_test_numerical]), axis=1)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(attbirnn_y_test, attbirnn_y_pred))\n",
    "\n",
    "cm = confusion_matrix(attbirnn_y_test, attbirnn_y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Concatenate, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "# Clean and process the original DataFrame `df`\n",
    "df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "df = df[df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "texts = df['text'].astype(str).tolist()\n",
    "approval_ratings = df['approval_rating'].tolist()\n",
    "user_reputations = df['user_reputation'].tolist()\n",
    "reply_counts = df['reply_count'].tolist()\n",
    "best_scores = df['best_score'].tolist()\n",
    "labels = df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_text, X_test_text, X_train_rating, X_test_rating, X_train_reputation, X_test_reputation, X_train_reply, X_test_reply, X_train_score, X_test_score, y_train_lstm_combined, y_test_lstm_combined = train_test_split(\n",
    "    texts, approval_ratings, user_reputations, reply_counts, best_scores, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize and Pad Text Data\n",
    "vocab_size = 5000\n",
    "max_seq_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_len, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_len, padding='post')\n",
    "\n",
    "# Normalize numerical features\n",
    "X_train_rating = np.array(X_train_rating).reshape(-1, 1) / 100.0\n",
    "X_test_rating = np.array(X_test_rating).reshape(-1, 1) / 100.0\n",
    "X_train_reputation = np.array(X_train_reputation).reshape(-1, 1) / df['user_reputation'].max()\n",
    "X_test_reputation = np.array(X_test_reputation).reshape(-1, 1) / df['user_reputation'].max()\n",
    "X_train_reply = np.array(X_train_reply).reshape(-1, 1) / df['reply_count'].max()\n",
    "X_test_reply = np.array(X_test_reply).reshape(-1, 1) / df['reply_count'].max()\n",
    "X_train_score = np.array(X_train_score).reshape(-1, 1) / df['best_score'].max()\n",
    "X_test_score = np.array(X_test_score).reshape(-1, 1) / df['best_score'].max()\n",
    "\n",
    "# Combine numerical features into a single array\n",
    "X_train_numerical = np.hstack((X_train_rating, X_train_reputation, X_train_reply, X_train_score))\n",
    "X_test_numerical = np.hstack((X_test_rating, X_test_reputation, X_test_reply, X_test_score))\n",
    "\n",
    "# Use a subset of the data for hyperparameter tuning\n",
    "subset_size = len(X_train_padded) // 2\n",
    "X_train_subset_text = X_train_padded[:subset_size]\n",
    "X_train_subset_num = X_train_numerical[:subset_size]\n",
    "y_train_subset_lstm_combined = y_train_lstm_combined[:subset_size]\n",
    "\n",
    "# Step 2: Define the Model for Tuning\n",
    "def build_lstm_model(hp):\n",
    "    text_input = Input(shape=(max_seq_len,), name=\"text_input\")\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=hp.Int(\"embedding_dim\", min_value=64, max_value=128, step=64),\n",
    "        input_length=max_seq_len\n",
    "    )(text_input)\n",
    "    lstm_layer = LSTM(\n",
    "        units=hp.Int(\"lstm_units\", min_value=64, max_value=128, step=64),\n",
    "        return_sequences=False\n",
    "    )(embedding_layer)\n",
    "    \n",
    "    numerical_input = Input(shape=(4,), name=\"numerical_input\")\n",
    "    concatenated = Concatenate()([lstm_layer, numerical_input])\n",
    "    \n",
    "    dense_layer = Dense(\n",
    "        units=hp.Int(\"dense_units\", min_value=64, max_value=128, step=64),\n",
    "        activation=\"relu\"\n",
    "    )(concatenated)\n",
    "    dropout_layer = Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.3, step=0.1))(dense_layer)\n",
    "    output_layer = Dense(3, activation=\"softmax\")(dropout_layer)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[text_input, numerical_input], outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-3, sampling=\"log\")\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 3: Hyperparameter Tuning with Keras Tuner\n",
    "temp_dir = tempfile.gettempdir()\n",
    "tuner = kt.Hyperband(\n",
    "    build_lstm_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=5,\n",
    "    factor=4,\n",
    "    directory=temp_dir,\n",
    "    project_name=\"lstm_sentiment_tuning_with_num\"\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "\n",
    "tuner.search(\n",
    "    [X_train_subset_text, X_train_subset_num],\n",
    "    np.array(y_train_subset_lstm_combined),\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 4: Get the Best Hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The optimal embedding dimension is {best_hps.get('embedding_dim')},\n",
    "with {best_hps.get('lstm_units')} LSTM units,\n",
    "{best_hps.get('dense_units')} units in the dense layer,\n",
    "dropout of {best_hps.get('dropout')},\n",
    "and learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Step 5: Train the Best Model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    [X_train_padded, X_train_numerical],\n",
    "    np.array(y_train_lstm_combined),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "test_loss_lstm_combined, test_accuracy_lstm_combined = best_model.evaluate(\n",
    "    [X_test_padded, X_test_numerical], \n",
    "    np.array(y_test_lstm_combined), \n",
    "    verbose=0\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy_lstm_combined}\")\n",
    "\n",
    "# Step 7: Predictions and Metrics\n",
    "y_pred_lstm_combined = np.argmax(best_model.predict([X_test_padded, X_test_numerical]), axis=1)\n",
    "\n",
    "# Compute macro F1-score\n",
    "macro_f1_lstm_combined = f1_score(y_test_lstm_combined, y_pred_lstm_combined, average='macro')\n",
    "print(f\"Macro F1-Score: {macro_f1_lstm_combined}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_lstm_combined = confusion_matrix(y_test_lstm_combined, y_pred_lstm_combined)\n",
    "sns.heatmap(cm_lstm_combined, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_lstm_combined, y_pred_lstm_combined, target_names=label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification model built using a Deep Neural Network (DNN) architecture with an Embedding Layer and a Global Average Pooling Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Filter valid data\n",
    "df = df.dropna(subset=['text', 'stars'])\n",
    "df = df[df['stars'].between(1, 5)]  # Ensure stars values are between 1 and 5\n",
    "\n",
    "# Step 2: Preprocess the text data\n",
    "dnn_texts = df['text'].astype(str)\n",
    "dnn_labels = df['stars']\n",
    "\n",
    "# Step 3: Split the dataset\n",
    "dnn_X_train, dnn_X_test, dnn_y_train, dnn_y_test = train_test_split(\n",
    "    dnn_texts, dnn_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Tokenize text data\n",
    "dnn_tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "dnn_tokenizer.fit_on_texts(dnn_X_train)\n",
    "\n",
    "dnn_X_train_seq = dnn_tokenizer.texts_to_sequences(dnn_X_train)\n",
    "dnn_X_test_seq = dnn_tokenizer.texts_to_sequences(dnn_X_test)\n",
    "\n",
    "# Step 5: Pad sequences\n",
    "dnn_max_len = 100\n",
    "dnn_X_train_padded = pad_sequences(dnn_X_train_seq, maxlen=dnn_max_len, padding='post', truncating='post')\n",
    "dnn_X_test_padded = pad_sequences(dnn_X_test_seq, maxlen=dnn_max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to 0-indexed (since stars are 1-5, subtract 1)\n",
    "dnn_y_train = dnn_y_train - 1\n",
    "dnn_y_test = dnn_y_test - 1\n",
    "\n",
    "# Step 6: Build the model\n",
    "dnn_num_classes = 5\n",
    "\n",
    "dnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add dropout for regularization\n",
    "    tf.keras.layers.Dense(dnn_num_classes, activation='softmax')  # Softmax for multi-class output\n",
    "])\n",
    "\n",
    "dnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # Multi-class loss function\n",
    "                  metrics=[])  # Remove accuracy to calculate F1-score separately\n",
    "\n",
    "# Step 7: Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=3,          # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restore the best weights after stopping\n",
    ")\n",
    "\n",
    "# Step 8: Train the model\n",
    "dnn_history = dnn_model.fit(\n",
    "    dnn_X_train_padded, \n",
    "    dnn_y_train, \n",
    "    epochs=20,  # Set a higher number of epochs\n",
    "    validation_data=(dnn_X_test_padded, dnn_y_test),\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]  # Add the early stopping callback\n",
    ")\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "# Predict classes for the test set\n",
    "dnn_y_test_pred_probs = dnn_model.predict(dnn_X_test_padded)\n",
    "dnn_y_test_pred = tf.argmax(dnn_y_test_pred_probs, axis=1).numpy()\n",
    "\n",
    "# Classification Report\n",
    "dnn_class_report = classification_report(dnn_y_test, dnn_y_test_pred, target_names=[str(i) for i in range(1, 6)])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(dnn_class_report)\n",
    "\n",
    "# Calculate F1-score\n",
    "dnn_f1 = f1_score(dnn_y_test, dnn_y_test_pred, average='weighted')\n",
    "print(f\"Test F1-Score: {dnn_f1:.4f}\")\n",
    "\n",
    "# Step 10: Plot training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dnn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(dnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.axvline(x=len(dnn_history.history['loss']) - early_stopping.patience - 1, color='red', linestyle='--', label='Early Stopping Triggered')\n",
    "plt.title('Model Loss with Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “loss” represents how far off the model’s predictions are from the true labels. Specifically:\n",
    "\t•\tTraining Loss: Measures the error the model makes on the training data.\n",
    "\t•\tValidation Loss: Measures the error the model makes on unseen (validation) data.\n",
    "    \n",
    "The graph shows that both training and validation loss decrease over time, meaning the model is learning effectively.\n",
    "\n",
    "The validation loss is slightly higher than the training loss, which is normal and indicates the model generalizes well. Since the losses flatten at the end and there’s no large gap, the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Prepare the DataFrame for the specific recipe\n",
    "df_specific = df.copy()\n",
    "\n",
    "# Filter for the specific recipe\n",
    "specific_recipe = \"Caramel Heavenlies\"\n",
    "df_specific = df_specific.dropna(subset=['text', 'stars', 'recipe_name'])\n",
    "df_specific = df_specific[df_specific['stars'].between(1, 5)]\n",
    "df_specific = df_specific[df_specific['recipe_name'] == specific_recipe]\n",
    "\n",
    "# Check if there are enough reviews for the recipe\n",
    "if df_specific.shape[0] < 1:\n",
    "    raise ValueError(f\"No reviews found for the recipe '{specific_recipe}'.\")\n",
    "\n",
    "# Preprocess the text and labels\n",
    "texts_specific = df_specific['text'].astype(str)\n",
    "labels_specific = df_specific['stars']\n",
    "\n",
    "# Handle small datasets\n",
    "if df_specific.shape[0] < 2:\n",
    "    print(f\"Warning: Only {df_specific.shape[0]} review(s) found for '{specific_recipe}'.\")\n",
    "    X_train, X_test, y_train, y_test = texts_specific, texts_specific, labels_specific, labels_specific\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts_specific, labels_specific, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 150\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Convert labels to 0-indexed\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1\n",
    "\n",
    "# Step 2: Build and Train the LSTM Model\n",
    "num_classes = 5\n",
    "model_specific = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_specific.compile(optimizer='adam',\n",
    "                       loss='sparse_categorical_crossentropy',\n",
    "                       metrics=[])\n",
    "\n",
    "# Train the model\n",
    "history_specific = model_specific.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Step 3: Evaluate with F1-Score\n",
    "# Predict and generate predicted classes\n",
    "y_pred_probs = model_specific.predict(X_test_padded)\n",
    "y_pred_classes = y_pred_probs.argmax(axis=-1)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(f\"{specific_recipe} Model Test F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Step 4: Classification Report\n",
    "# Dynamically set target names based on unique classes\n",
    "unique_classes = sorted(y_test.unique())\n",
    "target_names = [str(label + 1) for label in unique_classes]  # Adjust to match star ratings\n",
    "\n",
    "print(f\"\\n{specific_recipe} Model Classification Report\")\n",
    "print(classification_report(y_test, y_pred_classes, target_names=target_names))\n",
    "\n",
    "# Step 5: Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title(f'Confusion Matrix - {specific_recipe} Model')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Visualize Training and Validation Metrics\n",
    "# Training and Validation Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_specific.history['loss'], label='Training Loss')\n",
    "plt.plot(history_specific.history['val_loss'], label='Validation Loss')\n",
    "plt.title(f'{specific_recipe} Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_halving_search_cv  # Enable HalvingGridSearchCV\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, make_scorer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data cleaning and preprocessing\n",
    "rf_model_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "rf_model_df = rf_model_df[rf_model_df['stars'].between(0, 5)]  # Ensure stars are within valid range\n",
    "\n",
    "# Map stars to sentiment categories\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "rf_model_df['sentiment'] = rf_model_df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels into integers\n",
    "rf_sentiment_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "rf_model_df['sentiment_encoded'] = rf_model_df['sentiment'].map(rf_sentiment_label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "rf_model_texts = rf_model_df['text'].astype(str).tolist()\n",
    "rf_model_approval_ratings = rf_model_df['approval_rating'].tolist()\n",
    "rf_model_user_reputation = rf_model_df['user_reputation'].tolist()\n",
    "rf_model_reply_count = rf_model_df['reply_count'].tolist()\n",
    "rf_model_best_score = rf_model_df['best_score'].tolist()\n",
    "rf_model_labels = rf_model_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "rf_train_texts, rf_test_texts, rf_train_approval_ratings, rf_test_approval_ratings, rf_train_reputation, rf_test_reputation, \\\n",
    "rf_train_reply_count, rf_test_reply_count, rf_train_best_score, rf_test_best_score, rf_train_labels, rf_test_labels = train_test_split(\n",
    "    rf_model_texts, rf_model_approval_ratings, rf_model_user_reputation, rf_model_reply_count, rf_model_best_score, rf_model_labels,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Text feature extraction using TF-IDF\n",
    "rf_tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "rf_train_tfidf_features = rf_tfidf_vectorizer.fit_transform(rf_train_texts).toarray()\n",
    "rf_test_tfidf_features = rf_tfidf_vectorizer.transform(rf_test_texts).toarray()\n",
    "\n",
    "# Normalize numerical features\n",
    "rf_normalized_train_approval_ratings = np.array(rf_train_approval_ratings).reshape(-1, 1) / 100.0\n",
    "rf_normalized_test_approval_ratings = np.array(rf_test_approval_ratings).reshape(-1, 1) / 100.0\n",
    "rf_normalized_train_reputation = np.array(rf_train_reputation).reshape(-1, 1) / max(rf_model_user_reputation)\n",
    "rf_normalized_test_reputation = np.array(rf_test_reputation).reshape(-1, 1) / max(rf_model_user_reputation)\n",
    "rf_normalized_train_reply_count = np.array(rf_train_reply_count).reshape(-1, 1) / max(rf_model_reply_count)\n",
    "rf_normalized_test_reply_count = np.array(rf_test_reply_count).reshape(-1, 1) / max(rf_model_reply_count)\n",
    "rf_normalized_train_best_score = np.array(rf_train_best_score).reshape(-1, 1) / max(rf_model_best_score)\n",
    "rf_normalized_test_best_score = np.array(rf_test_best_score).reshape(-1, 1) / max(rf_model_best_score)\n",
    "\n",
    "# Combine all features\n",
    "rf_final_train_features = np.hstack((\n",
    "    rf_train_tfidf_features,\n",
    "    rf_normalized_train_approval_ratings,\n",
    "    rf_normalized_train_reputation,\n",
    "    rf_normalized_train_reply_count,\n",
    "    rf_normalized_train_best_score\n",
    "))\n",
    "rf_final_test_features = np.hstack((\n",
    "    rf_test_tfidf_features,\n",
    "    rf_normalized_test_approval_ratings,\n",
    "    rf_normalized_test_reputation,\n",
    "    rf_normalized_test_reply_count,\n",
    "    rf_normalized_test_best_score\n",
    "))\n",
    "\n",
    "# Parameter grid for hyperparameter tuning\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced']\n",
    "}\n",
    "\n",
    "# Custom scorer for macro F1\n",
    "rf_macro_f1_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# HalvingGridSearchCV for hyperparameter tuning\n",
    "rf_halving_grid_search = HalvingGridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    scoring=rf_macro_f1_scorer,  # Optimize for macro F1 score\n",
    "    factor=2,  # Controls the rate of resource reduction\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Early stopping mechanism for macro F1\n",
    "rf_best_macro_f1_score = 0\n",
    "rf_no_improvement_count = 0\n",
    "rf_patience = 3  # Number of rounds without improvement before stopping\n",
    "\n",
    "# Fit the model\n",
    "rf_halving_grid_search.fit(rf_final_train_features, rf_train_labels)\n",
    "\n",
    "# Access results and implement early stopping manually\n",
    "for idx in range(len(rf_halving_grid_search.cv_results_['mean_test_score'])):\n",
    "    rf_macro_f1_score = rf_halving_grid_search.cv_results_['mean_test_score'][idx]\n",
    "    print(f\"Iteration {idx + 1}: Macro F1 Score = {rf_macro_f1_score:.4f}\")\n",
    "\n",
    "    if rf_macro_f1_score > rf_best_macro_f1_score:\n",
    "        rf_best_macro_f1_score = rf_macro_f1_score\n",
    "        rf_no_improvement_count = 0\n",
    "    else:\n",
    "        rf_no_improvement_count += 1\n",
    "\n",
    "    if rf_no_improvement_count >= rf_patience:\n",
    "        print(\"Early stopping triggered during hyperparameter tuning.\")\n",
    "        break\n",
    "\n",
    "# Best parameters from HalvingGridSearchCV\n",
    "rf_best_rf_classifier = rf_halving_grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {rf_halving_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model\n",
    "rf_predicted_labels = rf_best_rf_classifier.predict(rf_final_test_features)\n",
    "rf_sentiment_accuracy = rf_best_rf_classifier.score(rf_final_test_features, rf_test_labels)\n",
    "rf_macro_f1 = f1_score(rf_test_labels, rf_predicted_labels, average='macro')\n",
    "print(f\"Test Accuracy: {rf_sentiment_accuracy:.2f}\")\n",
    "print(f\"Test Macro F1 Score: {rf_macro_f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix Visualization\n",
    "rf_sentiment_cm = confusion_matrix(rf_test_labels, rf_predicted_labels)\n",
    "sns.heatmap(rf_sentiment_cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=rf_sentiment_label_mapping.keys(), yticklabels=rf_sentiment_label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(rf_test_labels, rf_predicted_labels, target_names=rf_sentiment_label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Filter missing or invalid rows\n",
    "lstm_df = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "lstm_df = lstm_df[lstm_df['stars'].between(0, 5)]\n",
    "\n",
    "# Map stars to sentiment categories\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"positive\"\n",
    "\n",
    "lstm_df['sentiment'] = lstm_df['stars'].apply(map_star_to_sentiment)\n",
    "sentiment_label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "lstm_df['sentiment_encoded'] = lstm_df['sentiment'].map(sentiment_label_mapping)\n",
    "\n",
    "# Extract text, numerical features, and labels\n",
    "lstm_texts = lstm_df['text'].astype(str).tolist()\n",
    "lstm_approval_ratings = lstm_df['approval_rating'].tolist()\n",
    "lstm_user_reputation = lstm_df['user_reputation'].tolist()\n",
    "lstm_reply_count = lstm_df['reply_count'].tolist()\n",
    "lstm_best_score = lstm_df['best_score'].tolist()\n",
    "lstm_labels = lstm_df['sentiment_encoded'].tolist()\n",
    "\n",
    "# Step 2: Split into train and test sets\n",
    "lstm_X_train_text, lstm_X_test_text, lstm_X_train_approval, lstm_X_test_approval, \\\n",
    "lstm_X_train_reputation, lstm_X_test_reputation, lstm_X_train_reply, lstm_X_test_reply, \\\n",
    "lstm_X_train_score, lstm_X_test_score, lstm_y_train, lstm_y_test = train_test_split(\n",
    "    lstm_texts, lstm_approval_ratings, lstm_user_reputation, lstm_reply_count, lstm_best_score, lstm_labels,\n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize numerical features\n",
    "lstm_X_train_approval = np.array(lstm_X_train_approval).reshape(-1, 1) / 100.0\n",
    "lstm_X_test_approval = np.array(lstm_X_test_approval).reshape(-1, 1) / 100.0\n",
    "lstm_X_train_reputation = np.array(lstm_X_train_reputation).reshape(-1, 1) / max(lstm_user_reputation)\n",
    "lstm_X_test_reputation = np.array(lstm_X_test_reputation).reshape(-1, 1) / max(lstm_user_reputation)\n",
    "lstm_X_train_reply = np.array(lstm_X_train_reply).reshape(-1, 1) / max(lstm_reply_count)\n",
    "lstm_X_test_reply = np.array(lstm_X_test_reply).reshape(-1, 1) / max(lstm_reply_count)\n",
    "lstm_X_train_score = np.array(lstm_X_train_score).reshape(-1, 1) / max(lstm_best_score)\n",
    "lstm_X_test_score = np.array(lstm_X_test_score).reshape(-1, 1) / max(lstm_best_score)\n",
    "\n",
    "lstm_X_train_numerical = np.hstack((\n",
    "    lstm_X_train_approval, lstm_X_train_reputation, lstm_X_train_reply, lstm_X_train_score\n",
    "))\n",
    "lstm_X_test_numerical = np.hstack((\n",
    "    lstm_X_test_approval, lstm_X_test_reputation, lstm_X_test_reply, lstm_X_test_score\n",
    "))\n",
    "\n",
    "# Step 3: Tokenize the text data\n",
    "lstm_max_words = 5000\n",
    "lstm_max_len = 50\n",
    "\n",
    "lstm_tokenizer = Tokenizer(num_words=lstm_max_words, oov_token='<OOV>')\n",
    "lstm_tokenizer.fit_on_texts(lstm_X_train_text)\n",
    "\n",
    "lstm_X_train_seq = lstm_tokenizer.texts_to_sequences(lstm_X_train_text)\n",
    "lstm_X_test_seq = lstm_tokenizer.texts_to_sequences(lstm_X_test_text)\n",
    "\n",
    "lstm_X_train_padded = pad_sequences(lstm_X_train_seq, maxlen=lstm_max_len, padding='post', truncating='post')\n",
    "lstm_X_test_padded = pad_sequences(lstm_X_test_seq, maxlen=lstm_max_len, padding='post', truncating='post')\n",
    "\n",
    "# Step 4: Define the Bidirectional LSTM Model\n",
    "def build_lstm_model(units=64, dropout_rate=0.5, learning_rate=1e-3):\n",
    "    text_input = tf.keras.layers.Input(shape=(lstm_max_len,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=lstm_max_words, output_dim=64)(text_input)\n",
    "    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True))(embedding)\n",
    "    pooling = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "    \n",
    "    # Numerical input branch\n",
    "    numerical_input = tf.keras.layers.Input(shape=(lstm_X_train_numerical.shape[1],))\n",
    "    combined = tf.keras.layers.Concatenate()([pooling, numerical_input])\n",
    "    \n",
    "    # Dense layers\n",
    "    dense = tf.keras.layers.Dense(units, activation='relu')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate)(dense)\n",
    "    output = tf.keras.layers.Dense(len(np.unique(lstm_y_train)), activation='softmax')(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[text_input, numerical_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Step 5: Compute Class Weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(lstm_y_train),\n",
    "    y=lstm_y_train\n",
    ")\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Train the model with class weights\n",
    "best_macro_f1 = 0\n",
    "best_params = {}\n",
    "\n",
    "param_grid = {\n",
    "    'units': [64],  # Test fewer unit sizes\n",
    "    'dropout_rate': [0.3],  # Test fewer dropout rates\n",
    "    'learning_rate': [1e-3]  # Test one learning rate\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing configuration: {params}\")\n",
    "    lstm_model = build_lstm_model(**params)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    \n",
    "    history = lstm_model.fit(\n",
    "        [lstm_X_train_padded, lstm_X_train_numerical], np.array(lstm_y_train),\n",
    "        validation_data=([lstm_X_test_padded, lstm_X_test_numerical], np.array(lstm_y_test)),\n",
    "        epochs=5,  # Reduce maximum epochs\n",
    "        batch_size=16,  # Smaller batch size\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weights_dict,  # Add class weights here\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate Macro F1 score\n",
    "    lstm_y_pred_probs = lstm_model.predict([lstm_X_test_padded, lstm_X_test_numerical])\n",
    "    lstm_y_pred = lstm_y_pred_probs.argmax(axis=1)\n",
    "    macro_f1 = f1_score(lstm_y_test, lstm_y_pred, average='macro')\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_params = params\n",
    "        best_model = lstm_model\n",
    "\n",
    "# Step 6: Display the Best Configuration\n",
    "print(f\"\\nBest Configuration: {best_params}\")\n",
    "print(f\"Best Macro F1 Score: {best_macro_f1:.4f}\")\n",
    "\n",
    "# Final Evaluation\n",
    "lstm_y_pred = best_model.predict([lstm_X_test_padded, lstm_X_test_numerical]).argmax(axis=1)\n",
    "\n",
    "# Generate classification report with text labels\n",
    "target_names = list(sentiment_label_mapping.keys())\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(lstm_y_test, lstm_y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, GlobalMaxPooling1D, Embedding, Concatenate, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "df_model2 = df.dropna(subset=['text', 'stars', 'approval_rating', 'user_reputation', 'reply_count', 'best_score'])\n",
    "df_model2 = df_model2[df_model2['stars'].between(0, 5)]\n",
    "\n",
    "# Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "df_model2['sentiment'] = df_model2['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Encode sentiment labels\n",
    "label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "df_model2['sentiment_encoded'] = df_model2['sentiment'].map(label_mapping)\n",
    "\n",
    "# Extract features and labels\n",
    "texts = df_model2['text'].astype(str).tolist()\n",
    "approval_ratings = df_model2['approval_rating'].tolist()\n",
    "user_reputations = df_model2['user_reputation'].tolist()\n",
    "reply_counts = df_model2['reply_count'].tolist()\n",
    "best_scores = df_model2['best_score'].tolist()\n",
    "labels = df_model2['sentiment_encoded'].tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_text, X_test_text, X_train_rating, X_test_rating, X_train_reputation, X_test_reputation, X_train_reply, X_test_reply, X_train_score, X_test_score, y_train, y_test = train_test_split(\n",
    "    texts, approval_ratings, user_reputations, reply_counts, best_scores, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize and pad text data\n",
    "vocab_size_model2 = 5000\n",
    "max_seq_len_model2 = 100\n",
    "\n",
    "tokenizer_model2 = Tokenizer(num_words=vocab_size_model2)\n",
    "tokenizer_model2.fit_on_texts(texts)\n",
    "\n",
    "X_train_seq_model2 = tokenizer_model2.texts_to_sequences(X_train_text)\n",
    "X_test_seq_model2 = tokenizer_model2.texts_to_sequences(X_test_text)\n",
    "\n",
    "X_train_padded_model2 = pad_sequences(X_train_seq_model2, maxlen=max_seq_len_model2, padding='post')\n",
    "X_test_padded_model2 = pad_sequences(X_test_seq_model2, maxlen=max_seq_len_model2, padding='post')\n",
    "\n",
    "# Normalize numerical features\n",
    "X_train_rating_model2 = np.array(X_train_rating).reshape(-1, 1) / 100.0  # Normalize to 0-1 range\n",
    "X_test_rating_model2 = np.array(X_test_rating).reshape(-1, 1) / 100.0\n",
    "X_train_reputation_model2 = np.array(X_train_reputation).reshape(-1, 1) / df_model2['user_reputation'].max()\n",
    "X_test_reputation_model2 = np.array(X_test_reputation).reshape(-1, 1) / df_model2['user_reputation'].max()\n",
    "X_train_reply_model2 = np.array(X_train_reply).reshape(-1, 1) / df_model2['reply_count'].max()\n",
    "X_test_reply_model2 = np.array(X_test_reply).reshape(-1, 1) / df_model2['reply_count'].max()\n",
    "X_train_score_model2 = np.array(X_train_score).reshape(-1, 1) / df_model2['best_score'].max()\n",
    "X_test_score_model2 = np.array(X_test_score).reshape(-1, 1) / df_model2['best_score'].max()\n",
    "\n",
    "# Combine numerical features\n",
    "X_train_numerical_model2 = np.hstack((X_train_rating_model2, X_train_reputation_model2, X_train_reply_model2, X_train_score_model2))\n",
    "X_test_numerical_model2 = np.hstack((X_test_rating_model2, X_test_reputation_model2, X_test_reply_model2, X_test_score_model2))\n",
    "\n",
    "# Define the model with hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    text_input = Input(shape=(max_seq_len_model2,), name=\"text_input\")\n",
    "    embedding = Embedding(\n",
    "        input_dim=vocab_size_model2,\n",
    "        output_dim=hp.Int(\"embedding_dim\", min_value=64, max_value=128, step=64),  # Reduced range\n",
    "        input_length=max_seq_len_model2\n",
    "    )(text_input)\n",
    "    conv = Conv1D(\n",
    "        filters=hp.Int(\"filters\", min_value=32, max_value=64, step=32),  # Reduced range\n",
    "        kernel_size=hp.Choice(\"kernel_size\", values=[3, 5]),  # Fewer options\n",
    "        activation=\"relu\"\n",
    "    )(embedding)\n",
    "    pool = GlobalMaxPooling1D()(conv)\n",
    "\n",
    "    # Numerical features input\n",
    "    numerical_input = Input(shape=(4,), name=\"numerical_input\")\n",
    "    combined = Concatenate()([pool, numerical_input])\n",
    "\n",
    "    # Fully connected layers\n",
    "    fc1 = Dense(hp.Int(\"units\", min_value=64, max_value=128, step=64), activation=\"relu\")(combined)  # Reduced range\n",
    "    dropout = Dropout(hp.Float(\"dropout\", min_value=0.1, max_value=0.3, step=0.1))(fc1)  # Reduced range\n",
    "    output = Dense(3, activation=\"softmax\", name=\"output\")(dropout)\n",
    "\n",
    "    model = Model(inputs=[text_input, numerical_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-3, sampling=\"log\")  # Narrow range\n",
    "        ),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "import tempfile\n",
    "\n",
    "# Use a temporary writable directory for Keras Tuner\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "# Create the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=5,  # Reduced epochs\n",
    "    factor=4,  # Increased reduction factor to reduce trials\n",
    "    directory=temp_dir,  # Temporary directory for tuner data\n",
    "    project_name=\"cnn_tuning_model2\"\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search with a subset of data\n",
    "subset_size = len(X_train_padded_model2) // 2  # Use 50% of the training data\n",
    "tuner.search(\n",
    "    [X_train_padded_model2[:subset_size], X_train_numerical_model2[:subset_size]],\n",
    "    np.array(y_train[:subset_size]),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]  # Short patience\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search with a subset of data\n",
    "subset_size = len(X_train_padded_model2) // 2  # Use 50% of the training data\n",
    "tuner.search(\n",
    "    [X_train_padded_model2[:subset_size], X_train_numerical_model2[:subset_size]],\n",
    "    np.array(y_train[:subset_size]),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]  # Short patience\n",
    ")\n",
    "\n",
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The optimal embedding dimension is {best_hps.get('embedding_dim')},\n",
    "with {best_hps.get('filters')} filters, kernel size of {best_hps.get('kernel_size')},\n",
    "{best_hps.get('units')} units in the dense layer, dropout of {best_hps.get('dropout')},\n",
    "and learning rate of {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Train the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    [X_train_padded_model2, X_train_numerical_model2],\n",
    "    np.array(y_train),\n",
    "    validation_split=0.2,\n",
    "    epochs=5,  # Reduced epochs\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=2)]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = best_model.evaluate(\n",
    "    [X_test_padded_model2, X_test_numerical_model2], \n",
    "    np.array(y_test)\n",
    ")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = np.argmax(best_model.predict([X_test_padded_model2, X_test_numerical_model2]), axis=1)\n",
    "\n",
    "# Compute macro F1-score\n",
    "macro_f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(f\"Macro F1-Score: {macro_f1}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Star Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Filter missing or invalid rows\n",
    "bow_df = df.dropna(subset=['text', 'stars'])\n",
    "bow_df = bow_df[bow_df['stars'].between(0, 5)]  # Ensure stars range is 1-5\n",
    "\n",
    "# Step 2: Preprocess text and labels\n",
    "bow_texts = bow_df['text'].astype(str)\n",
    "bow_labels = bow_df['stars'] - 1  # Convert labels to 0-based indexing\n",
    "\n",
    "# Step 3: Split into train and test sets\n",
    "bow_X_train, bow_X_test, bow_y_train, bow_y_test = train_test_split(\n",
    "    bow_texts, bow_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 4: Vectorize the text data using Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=5000)  # Limit to top 5000 words\n",
    "bow_X_train_vec = vectorizer.fit_transform(bow_X_train)\n",
    "bow_X_test_vec = vectorizer.transform(bow_X_test)\n",
    "\n",
    "# Step 5: Train a Logistic Regression model\n",
    "classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "classifier.fit(bow_X_train_vec, bow_y_train)\n",
    "\n",
    "# Step 6: Make predictions\n",
    "bow_y_pred = classifier.predict(bow_X_test_vec)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "f1 = f1_score(bow_y_test, bow_y_pred, average='weighted')\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(bow_y_test, bow_y_pred, target_names=[f\"{i+1}-Star\" for i in range(5)]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(bow_y_test, bow_y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[f\"{i+1}-Star\" for i in range(5)], yticklabels=[f\"{i+1}-Star\" for i in range(5)])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Bag of Words Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams and Trigrams by Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Step 1: Ensure you have the necessary NLTK stopwords installed\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 2: Define a custom stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Extend with custom stop words and common contractions\n",
    "additional_stop_words = {'39', 've', 'don', 'll', 'didn', 'doesn', 'wasn', 'won', 'like', 'make', 'good', 'recipe', 'followed', 'next time'}\n",
    "stop_words.update(additional_stop_words)\n",
    "\n",
    "# Step 3: Preprocessing function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text by removing punctuation, lowercasing, and filtering stopwords.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove non-alphabetic characters\n",
    "    words = text.lower().split()\n",
    "    return \" \".join([word for word in words if word not in stop_words])\n",
    "\n",
    "# Step 4: Clean the text data for sentiment analysis\n",
    "df_model2['cleaned_text'] = df_model2['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Step 5: Function for extracting bigrams and trigrams\n",
    "def get_ngrams(corpus, ngram_range=(2, 3), top_n=20):\n",
    "    \"\"\"\n",
    "    Extract top n bigrams and trigrams from the corpus.\n",
    "\n",
    "    Args:\n",
    "    - corpus (list of str): Text data\n",
    "    - ngram_range (tuple): Range of ngrams to extract (e.g., (2, 3) for bigrams and trigrams)\n",
    "    - top_n (int): Number of top ngrams to return\n",
    "\n",
    "    Returns:\n",
    "    - Counter: Counter object with top ngrams and their frequencies\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range)\n",
    "    ngram_matrix = vectorizer.fit_transform(corpus)\n",
    "    ngram_counts = ngram_matrix.sum(axis=0).A1\n",
    "    ngram_vocab = vectorizer.get_feature_names_out()\n",
    "    ngram_counter = Counter(dict(zip(ngram_vocab, ngram_counts)))\n",
    "    # Remove 'next time' ngram\n",
    "    ngram_counter = Counter({key: count for key, count in ngram_counter.items() if 'next time' not in key})\n",
    "    return ngram_counter.most_common(top_n)\n",
    "\n",
    "# Step 6: Generate bigrams and trigrams for each sentiment label\n",
    "for sentiment in sorted(df_model2['sentiment'].unique()):\n",
    "    sentiment_texts = df_model2[df_model2['sentiment'] == sentiment]['cleaned_text']\n",
    "    print(f\"\\nTop Bigrams and Trigrams for '{sentiment}' Sentiment:\")\n",
    "    top_ngrams = get_ngrams(sentiment_texts, ngram_range=(2, 3), top_n=10)\n",
    "    for ngram, count in top_ngrams:\n",
    "        print(f\"{ngram}: {count}\")\n",
    "\n",
    "# Step 7 (Optional): Visualize ngrams for a specific sentiment\n",
    "def plot_ngrams(sentiment_label, ngram_range=(2, 3), top_n=10):\n",
    "    \"\"\"\n",
    "    Plot the top ngrams for a specific sentiment.\n",
    "\n",
    "    Args:\n",
    "    - sentiment_label (str): The sentiment label to filter on ('negative', 'neutral', or 'positive')\n",
    "    - ngram_range (tuple): Range of ngrams to extract\n",
    "    - top_n (int): Number of top ngrams to visualize\n",
    "    \"\"\"\n",
    "    sentiment_texts = df_model2[df_model2['sentiment'] == sentiment_label]['cleaned_text']\n",
    "    top_ngrams = get_ngrams(sentiment_texts, ngram_range=ngram_range, top_n=top_n)\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    ngrams, counts = zip(*top_ngrams)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(ngrams, counts, color='skyblue')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Ngrams')\n",
    "    plt.title(f'Top {top_n} Bigrams and Trigrams for {sentiment_label.capitalize()} Sentiment')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot for 'positive' sentiment\n",
    "plot_ngrams(sentiment_label='positive', ngram_range=(2, 3), top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display all Recipes List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Get the value counts of 'recipe_name' as a DataFrame\n",
    "recipe_counts = df['recipe_name'].value_counts().reset_index()\n",
    "recipe_counts.columns = ['Recipe Name', 'Count']\n",
    "\n",
    "# Convert the DataFrame to a scrollable HTML element\n",
    "html_table = recipe_counts.to_html(index=False)\n",
    "scrollable_table = f\"\"\"\n",
    "<div style=\"height:400px; overflow:auto; border:1px solid black; padding:10px;\">\n",
    "{html_table}\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Display the scrollable table\n",
    "display(HTML(scrollable_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA & Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Parse 'created_at' column as datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Step 2: Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "df['sentiment'] = df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Step 3: Aggregate data by sentiment proportions (daily)\n",
    "daily_sentiment = df.groupby(pd.Grouper(key='created_at', freq='D'))['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Convert proportions to percentages for better readability\n",
    "daily_sentiment *= 100\n",
    "\n",
    "# Step 4: Plot time series for each sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "for sentiment in daily_sentiment.columns:\n",
    "    plt.plot(daily_sentiment.index, daily_sentiment[sentiment], label=f\"{sentiment.capitalize()} Sentiment\")\n",
    "plt.axvline(daily_sentiment.index[-30], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title('Daily Sentiment Percentages Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for Prophet (example: 'positive' sentiment)\n",
    "prophet_data_positive = daily_sentiment['positive'].reset_index().rename(columns={'created_at': 'ds', 'positive': 'y'})\n",
    "\n",
    "# Step 6: Fit Prophet Model for 'positive' sentiment\n",
    "positive_prophet_model = Prophet()\n",
    "positive_prophet_model.fit(prophet_data_positive)\n",
    "\n",
    "# Step 7: Make future predictions with Prophet\n",
    "positive_future = positive_prophet_model.make_future_dataframe(periods=30)  # Forecast next 30 days\n",
    "positive_forecast = positive_prophet_model.predict(positive_future)\n",
    "\n",
    "# Extract Prophet forecasted values for the next 30 days\n",
    "positive_forecast_next_30 = positive_forecast[['ds', 'yhat']].tail(30)\n",
    "positive_forecast_next_30['yhat'] = positive_forecast_next_30['yhat'].clip(lower=0, upper=100)  # Ensure valid percentages\n",
    "\n",
    "# Step 8: Use SARIMA for time series modeling on 'positive' sentiment\n",
    "# Ensure the DataFrame uses the datetime index\n",
    "daily_sentiment.index = pd.to_datetime(daily_sentiment.index)\n",
    "\n",
    "# Fit SARIMA model for 'positive' sentiment with the corrected index\n",
    "sarima_model_positive = SARIMAX(\n",
    "    daily_sentiment['positive'], \n",
    "    order=(1, 1, 1), \n",
    "    seasonal_order=(1, 1, 1, 7)  # Weekly seasonality\n",
    ")\n",
    "sarima_positive_result = sarima_model_positive.fit(disp=False)\n",
    "\n",
    "# Forecast the next 30 days using SARIMA with proper datetime alignment\n",
    "sarima_forecast_next_30_positive = sarima_positive_result.get_forecast(steps=30)\n",
    "\n",
    "# Use the correct forecasted dates from the index\n",
    "forecast_index = pd.date_range(\n",
    "    start=daily_sentiment.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=30, \n",
    "    freq='D'\n",
    ")\n",
    "sarima_forecast_summary_positive = sarima_forecast_next_30_positive.summary_frame()\n",
    "sarima_forecast_summary_positive.index = forecast_index  # Align dates with the forecast\n",
    "\n",
    "# Ensure the forecasts are clipped to valid percentages\n",
    "sarima_forecast_summary_positive['mean'] = sarima_forecast_summary_positive['mean'].clip(lower=0, upper=100)\n",
    "\n",
    "# Step 9: Plot combined actuals and forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sentiment.index, daily_sentiment['positive'], label='Actual Positive Sentiment (%)', color='blue')\n",
    "plt.plot(positive_forecast_next_30['ds'], positive_forecast_next_30['yhat'], label='Prophet Forecast (%)', linestyle='--', color='green')\n",
    "plt.plot(sarima_forecast_summary_positive.index, sarima_forecast_summary_positive['mean'], label='SARIMA Forecast (%)', linestyle='--', color='orange')\n",
    "plt.axvline(daily_sentiment.index[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title('Positive Sentiment Forecast: Prophet vs SARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Combine Prophet and SARIMA forecasts into a summary table\n",
    "forecast_summary_positive = pd.DataFrame({\n",
    "    'Date': positive_forecast_next_30['ds'],\n",
    "    'Prophet Forecast (%)': positive_forecast_next_30['yhat'],\n",
    "    'SARIMA Forecast (%)': sarima_forecast_summary_positive['mean'].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the summary table\n",
    "print(\"\\nPositive Sentiment Forecast (Next 30 Days):\")\n",
    "print(forecast_summary_positive.to_string(index=False))\n",
    "\n",
    "# Step 11: Add inline commentary\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Prophet predicts an average positive sentiment of {positive_forecast_next_30['yhat'].mean():.2f}% over the next 30 days.\")\n",
    "print(f\"SARIMA predicts an average positive sentiment of {sarima_forecast_summary_positive['mean'].mean():.2f}% over the same period.\")\n",
    "print(\"Both models suggest relatively stable positive sentiment trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting Sentiment for Specific Recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Parse 'created_at' column as datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Step 2: Filter for a specific recipe\n",
    "specific_recipe_name = 'Cheeseburger Soup'  # Change this to your desired recipe\n",
    "recipe_df = df[df['recipe_name'] == specific_recipe_name]\n",
    "\n",
    "# Step 3: Map stars to sentiment\n",
    "def map_star_to_sentiment(stars):\n",
    "    if stars <= 2:\n",
    "        return \"negative\"\n",
    "    elif stars == 3:\n",
    "        return \"neutral\"\n",
    "    else:  # 4 or 5 stars\n",
    "        return \"positive\"\n",
    "\n",
    "recipe_df['sentiment'] = recipe_df['stars'].apply(map_star_to_sentiment)\n",
    "\n",
    "# Step 4: Aggregate data by sentiment proportions (daily)\n",
    "daily_sentiment_recipe = recipe_df.groupby(pd.Grouper(key='created_at', freq='D'))['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Convert proportions to percentages for better readability\n",
    "daily_sentiment_recipe *= 100\n",
    "\n",
    "# Step 5: Plot time series for each sentiment\n",
    "plt.figure(figsize=(12, 6))\n",
    "for sentiment in daily_sentiment_recipe.columns:\n",
    "    plt.plot(daily_sentiment_recipe.index, daily_sentiment_recipe[sentiment], label=f\"{sentiment.capitalize()} Sentiment\")\n",
    "plt.axvline(daily_sentiment_recipe.index[-30], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f'Daily Sentiment Percentages for {specific_recipe_name}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Prepare data for Prophet (example: 'positive' sentiment)\n",
    "prophet_data_positive_recipe = daily_sentiment_recipe['positive'].reset_index().rename(columns={'created_at': 'ds', 'positive': 'y'})\n",
    "\n",
    "# Step 7: Fit Prophet Model for 'positive' sentiment\n",
    "positive_prophet_model_recipe = Prophet()\n",
    "positive_prophet_model_recipe.fit(prophet_data_positive_recipe)\n",
    "\n",
    "# Step 8: Make future predictions with Prophet\n",
    "positive_future_recipe = positive_prophet_model_recipe.make_future_dataframe(periods=30)  # Forecast next 30 days\n",
    "positive_forecast_recipe = positive_prophet_model_recipe.predict(positive_future_recipe)\n",
    "\n",
    "# Extract Prophet forecasted values for the next 30 days\n",
    "positive_forecast_next_30_recipe = positive_forecast_recipe[['ds', 'yhat']].tail(30)\n",
    "positive_forecast_next_30_recipe['yhat'] = positive_forecast_next_30_recipe['yhat'].clip(lower=0, upper=100)  # Ensure valid percentages\n",
    "\n",
    "# Step 9: Use SARIMA for time series modeling on 'positive' sentiment\n",
    "# Ensure the DataFrame uses the datetime index\n",
    "daily_sentiment_recipe.index = pd.to_datetime(daily_sentiment_recipe.index)\n",
    "\n",
    "# Fit SARIMA model for 'positive' sentiment with the corrected index\n",
    "sarima_model_positive_recipe = SARIMAX(\n",
    "    daily_sentiment_recipe['positive'], \n",
    "    order=(1, 1, 1), \n",
    "    seasonal_order=(1, 1, 1, 7)  # Weekly seasonality\n",
    ")\n",
    "sarima_positive_result_recipe = sarima_model_positive_recipe.fit(disp=False)\n",
    "\n",
    "# Forecast the next 30 days using SARIMA with proper datetime alignment\n",
    "sarima_forecast_next_30_positive_recipe = sarima_positive_result_recipe.get_forecast(steps=30)\n",
    "\n",
    "# Use the correct forecasted dates from the index\n",
    "forecast_index_recipe = pd.date_range(\n",
    "    start=daily_sentiment_recipe.index[-1] + pd.Timedelta(days=1), \n",
    "    periods=30, \n",
    "    freq='D'\n",
    ")\n",
    "sarima_forecast_summary_positive_recipe = sarima_forecast_next_30_positive_recipe.summary_frame()\n",
    "sarima_forecast_summary_positive_recipe.index = forecast_index_recipe  # Align dates with the forecast\n",
    "\n",
    "# Ensure the forecasts are clipped to valid percentages\n",
    "sarima_forecast_summary_positive_recipe['mean'] = sarima_forecast_summary_positive_recipe['mean'].clip(lower=0, upper=100)\n",
    "\n",
    "# Step 10: Plot combined actuals and forecasts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(daily_sentiment_recipe.index, daily_sentiment_recipe['positive'], label='Actual Positive Sentiment (%)', color='blue')\n",
    "plt.plot(positive_forecast_next_30_recipe['ds'], positive_forecast_next_30_recipe['yhat'], label='Prophet Forecast (%)', linestyle='--', color='green')\n",
    "plt.plot(sarima_forecast_summary_positive_recipe.index, sarima_forecast_summary_positive_recipe['mean'], label='SARIMA Forecast (%)', linestyle='--', color='orange')\n",
    "plt.axvline(daily_sentiment_recipe.index[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f'Positive Sentiment Forecast for {specific_recipe_name}: Prophet vs SARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sentiment Percentage (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Combine Prophet and SARIMA forecasts into a summary table\n",
    "forecast_summary_positive_recipe = pd.DataFrame({\n",
    "    'Date': positive_forecast_next_30_recipe['ds'],\n",
    "    'Prophet Forecast (%)': positive_forecast_next_30_recipe['yhat'],\n",
    "    'SARIMA Forecast (%)': sarima_forecast_summary_positive_recipe['mean'].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Print the summary table\n",
    "print(f\"\\nPositive Sentiment Forecast for {specific_recipe_name} (Next 30 Days):\")\n",
    "print(forecast_summary_positive_recipe.to_string(index=False))\n",
    "\n",
    "# Step 12: Add inline commentary\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"Prophet predicts an average positive sentiment of {positive_forecast_next_30_recipe['yhat'].mean():.2f}% over the next 30 days for {specific_recipe_name}.\")\n",
    "print(f\"SARIMA predicts an average positive sentiment of {sarima_forecast_summary_positive_recipe['mean'].mean():.2f}% over the same period.\")\n",
    "print(\"Both models suggest relatively stable positive sentiment trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast Specific Recipe Star Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "\n",
    "# Step 1: Filter dataset for a specific recipe\n",
    "recipe_name_filter = \"Cheeseburger Soup\"\n",
    "filtered_df = df[df['recipe_name'] == recipe_name_filter]\n",
    "\n",
    "# Ensure the filtered DataFrame is not empty\n",
    "if filtered_df.empty:\n",
    "    raise ValueError(f\"No data available for the recipe: {recipe_name_filter}\")\n",
    "\n",
    "# Inspect recent data trends\n",
    "print(f\"\\nRecent Star Ratings for {recipe_name_filter}:\")\n",
    "print(filtered_df[['created_at', 'stars']].sort_values(by='created_at', ascending=False).head(10))\n",
    "\n",
    "# Step 2: Parse 'created_at' column as datetime\n",
    "filtered_df['created_at'] = pd.to_datetime(filtered_df['created_at'], errors='coerce')\n",
    "filtered_df = filtered_df.dropna(subset=['created_at'])  # Drop rows with invalid dates\n",
    "\n",
    "# Step 3: Aggregate data by date (e.g., daily mean of 'stars')\n",
    "recipe_ts = (\n",
    "    filtered_df.groupby(pd.Grouper(key='created_at', freq='D'))['stars']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ensure there are no missing dates and fill gaps\n",
    "recipe_ts['stars'] = recipe_ts['stars'].interpolate().fillna(0)\n",
    "\n",
    "# Step 4: Plot the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(recipe_ts['created_at'], recipe_ts['stars'], label=f'{recipe_name_filter} Star Ratings', color='blue')\n",
    "plt.title(f'Time Series of Star Ratings for {recipe_name_filter}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Prepare data for Prophet\n",
    "prophet_data = recipe_ts.rename(columns={'created_at': 'ds', 'stars': 'y'})\n",
    "\n",
    "# Step 6: Fit Prophet model\n",
    "recipe_prophet_model = Prophet()\n",
    "recipe_prophet_model.fit(prophet_data)\n",
    "\n",
    "# Step 7: Make future predictions\n",
    "future = recipe_prophet_model.make_future_dataframe(periods=30)\n",
    "forecast = recipe_prophet_model.predict(future)\n",
    "\n",
    "# Step 8: Rename the forecast columns for clarity (after Prophet operations)\n",
    "forecast_renamed = forecast.rename(columns={\n",
    "    'ds': 'Date',\n",
    "    'yhat': 'Predicted Star Rating',\n",
    "    'yhat_lower': 'Lower Confidence Bound',\n",
    "    'yhat_upper': 'Upper Confidence Bound'\n",
    "})\n",
    "\n",
    "# Step 9: Plot the forecast with forecasted values highlighted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(prophet_data['ds'], prophet_data['y'], label='Historical Data', color='blue')\n",
    "plt.plot(forecast['ds'], forecast['yhat'], label='Forecasted Ratings', color='orange', linestyle='--')\n",
    "plt.fill_between(\n",
    "    forecast['ds'], \n",
    "    forecast['yhat_lower'], \n",
    "    forecast['yhat_upper'], \n",
    "    color='orange', \n",
    "    alpha=0.2, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "plt.axvline(x=prophet_data['ds'].iloc[-1], color='red', linestyle='--', label='Forecast Start')\n",
    "plt.title(f\"Star Ratings Forecast for {recipe_name_filter}\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Star Rating')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Seasonal decomposition (keep 'ds' column for Prophet compatibility)\n",
    "fig2 = recipe_prophet_model.plot_components(forecast)\n",
    "plt.show()\n",
    "\n",
    "clear_forecasted_ratings = forecast_renamed[['Date', 'Predicted Star Rating', 'Lower Confidence Bound', 'Upper Confidence Bound']].tail(30)\n",
    "print(\"\\nForecasted Ratings\")\n",
    "print(clear_forecasted_ratings.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
